{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "znFeNzp9hOei"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e7dud7e/DAND/blob/master/L3/3_Multi_turn_chat_with_Llama_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most amazing capabilities of LLMs is to have human-like context-aware natural conversation. But LLMs are stateless, so unless the chat history is passed, LLMs won't be able to answer correctly follow up quetions. In this lesson, you'll explore how to chat with Llama 2 in a context-aware fashion."
      ],
      "metadata": {
        "id": "iudy4Dr20AF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup and helper**"
      ],
      "metadata": {
        "id": "rPF3VUL6MjSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHGPVbyu_mf9",
        "outputId": "c210aca9-df1c-47cb-e3c4-6350f501c888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #!echo \"TOGETHER_API_KEY=<your_together_api_key>\" >> .env # for local jupyter\n",
        " #!echo \"TOGETHER_API_KEY=<your_together_api_key>\" >> /content/.env # for google colab"
      ],
      "metadata": {
        "id": "4i9xGFdEZTa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import warnings\n",
        "\n",
        "_ = load_dotenv(find_dotenv())\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "TOGETHER_API_KEY = os.environ['TOGETHER_API_KEY']"
      ],
      "metadata": {
        "id": "mD3Z5HAH9YeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://api.together.xyz/inference\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def llama(query, add_inst=True, model=\"togethercomputer/llama-2-7b-chat\", temperature=0.0, max_token=256):\n",
        "  if add_inst:\n",
        "    prompt = f\"[INST]{query}[/INST]\"\n",
        "  else:\n",
        "    prompt = query\n",
        "\n",
        "  print(f\"Prompt:\\n{prompt}\\n\")\n",
        "\n",
        "  data = {\n",
        "      \"model\": model,\n",
        "      \"prompt\": prompt,\n",
        "      \"temperature\": temperature,\n",
        "      \"max_tokens\": max_token\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, headers=headers, json=data)\n",
        "  print(\"Response:\")\n",
        "\n",
        "  return response.json()['output']['choices'][0]['text']"
      ],
      "metadata": {
        "id": "F6Xr2byeVnr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Provide chat history**\n",
        "Store and pass previous context (prompt and response) along with new prompt, first in an intuitive way by setting a list of \"User\" and \"Assistant\" values:\n"
      ],
      "metadata": {
        "id": "D3Iq0-66XJxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"The typical color of a llama is what? Answer in a single word.\"\n",
        "response_1 = \"Brown\"\n",
        "prompt_2 = \"What colors are not typical?\"\n",
        "\n",
        "prompt_chat = f\"\"\"\n",
        "User: {prompt_1}\n",
        "Assistant: {response_1}\n",
        "User: {prompt_2}\n",
        "\"\"\"\n",
        "response_2 = llama(prompt_chat, model=\"togethercomputer/llama-2-7b-chat\")\n",
        "print(response_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA3Lb14SX-ZK",
        "outputId": "916fda23-6534-4167-fff0-7eb948a7c0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [INST]\n",
            "User: The typical color of a llama is what? Answer in a single word.\n",
            "Assistant: Brown\n",
            "User: What colors are not typical?\n",
            "[/INST]\n",
            "\n",
            "\n",
            " Sure! Here's the answer to your question:\n",
            "\n",
            "The typical color of a llama is brown. However, some llamas can also be found in other colors such as white, gray, and even spotted or patchy patterns. So, the colors that are not typical for a llama are white, gray, and any other non-brown colors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see by passing the context, Llama 2 can now correctly answer your follow up question. Let's continue the chat with another follow up question:"
      ],
      "metadata": {
        "id": "wTwJZO6gZqt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"The typical color of a llama is what? Answer in a single word.\"\n",
        "response_1 = \"Brown\"\n",
        "prompt_2 = \"What colors are not typical?\"\n",
        "repoonse_2 = \" Sure! Here's the answer to your question:\\n\\nThe typical color of a llama is brown. However, some llamas can also be found in other colors such as white, gray, and even spotted or patchy patterns. So, the colors that are not typical for a llama are white, gray, and any other non-brown colors.\"\n",
        "prompt_3 = \"Summarize the last answer with a few words.\"\n",
        "\n",
        "prompt_chat = f\"\"\"\n",
        "User: {prompt_1}\n",
        "Assistant: {response_1}\n",
        "User: {prompt_2}\n",
        "Assistant: {response_2}\n",
        "User: {prompt_3}\n",
        "\"\"\"\n",
        "response_3 = llama(prompt_chat, model=\"togethercomputer/llama-2-13b-chat\")\n",
        "print(response_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRMDF54QaGl2",
        "outputId": "5e5c2885-d5de-4899-df79-982a56a5dbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [INST]\n",
            "User: The typical color of a llama is what? Answer in a single word.\n",
            "Assistant: Brown\n",
            "User: What colors are not typical?\n",
            "Assistant:  Sure! Here's the answer to your question:\n",
            "\n",
            "The typical color of a llama is brown. However, some llamas can also be found in other colors such as white, gray, and even spotted or patchy patterns. So, the colors that are not typical for a llama are white, gray, and any other non-brown colors.\n",
            "User: Summarize the last answer with a few words.\n",
            "[/INST]\n",
            "\n",
            "\n",
            " Sure! Here's a summary of the last answer:\n",
            "\n",
            "\"Llamas are typically brown, but can also be white, gray, or have spotted patterns.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Use the recommended prompt format**\n",
        "\n",
        "For context-aware chat, the recommended Llama 2 multi-turn prompt format is as follows - you should use the recommended format even though the previous intuitive way may work in some cases:\n",
        "```\n",
        "<s>[INST] {{ user_prompt_1 }} [/INST]\n",
        "{{ model_response_1 }} </s>\n",
        "<s>[INST] {{ user_prompt_2 }} [/INST]\n",
        "{{ model_response_2 }} </s>\n",
        "... ...\n",
        "<s>[INST] {{ last_user_prompt }} [/INST]\n",
        "```"
      ],
      "metadata": {
        "id": "EY7dj8ON8Mvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's manually construct a prompt in the recommended format using the same prompts and responses:"
      ],
      "metadata": {
        "id": "znFeNzp9hOei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"The typical color of a llama is what? Answer in a single word.\"\n",
        "response_1 = \"Brown\"\n",
        "prompt_2 = \"What colors are not typical?\"\n",
        "prompt_chat = f\"\"\"\n",
        "<s>[INST] {prompt_1} [/INST]\n",
        "{response_1} </s>\n",
        "<s>[INST] {prompt_2} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# since the prompt_chat already uses the [INST] tag, set the add_inst to False\n",
        "response_2 = llama(prompt_chat, False)\n",
        "print(response_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jv7o-rwhQyr",
        "outputId": "0d8d671f-9038-448f-d4f6-ca4434aff73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \n",
            "<s>[INST] The typical color of a llama is what? Answer in a single word. [/INST]\n",
            "Brown </s>\n",
            "<s>[INST] What colors are not typical? [/INST]\n",
            "\n",
            "\n",
            "\n",
            "There are several colors that are not typical for llamas, including:\n",
            "\n",
            "* White\n",
            "* Gray\n",
            "* Black\n",
            "* Spotted or patchy patterns\n",
            "* Albino (pale pink or white skin and red eyes)\n",
            "* Dilute colors (such as cream, champagne, or isabella)\n",
            "\n",
            "It's worth noting that while these colors are not typical for llamas, they can still occur in the breed and are considered rare or unusual.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper function to format the multi-turn chats"
      ],
      "metadata": {
        "id": "6MWXXEhGi0g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt_chat(prompts, responses):\n",
        "  formatted_prompt = f\"<s>[INST] {prompts[0]} [/INST]\"\n",
        "  for n, response in enumerate(responses):\n",
        "    prompt = prompts[n + 1]\n",
        "    formatted_prompt += f\"\\n{response}\\n </s><s>[INST] \\n{ prompt }\\n [/INST]\"\n",
        "\n",
        "  return formatted_prompt"
      ],
      "metadata": {
        "id": "2VerzOmd8PLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the helper now:"
      ],
      "metadata": {
        "id": "jLDi4bzYj8iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"The typical color of a llama is what? Answer in a single word.\"\n",
        "response_1 = \"Brown\"\n",
        "prompt_2 = \"What colors are not typical?\"\n",
        "\n",
        "prompts = [prompt_1, prompt_2]\n",
        "responses = [response_1]\n",
        "\n",
        "response_2 = llama(get_prompt_chat(prompts, responses), False)\n",
        "print(response_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gijb1AFHfNQW",
        "outputId": "168abc36-02ec-4df5-9bff-ed1357609de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: <s>[INST] The typical color of a llama is what? Answer in a single word. [/INST]\n",
            "Brown\n",
            " </s><s>[INST] \n",
            "What colors are not typical?\n",
            " [/INST]\n",
            "\n",
            "\n",
            " While the typical color of a llama is brown, there are some colors that are not typical or less common, including:\n",
            "\n",
            "* White\n",
            "* Gray\n",
            "* Black\n",
            "* Cream\n",
            "* Roan (a mix of white and brown)\n",
            "* Palomino (a light golden color)\n",
            "* Dun (a mix of red and white)\n",
            "\n",
            "These colors are less common in llamas than the typical brown color, but they can still be found in some breeds and individuals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For further follow up question:"
      ],
      "metadata": {
        "id": "1OPnoJL-klva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"The typical color of a llama is what? Answer in a single word.\"\n",
        "response_1 = \"Brown\"\n",
        "prompt_2 = \"What colors are not typical?\"\n",
        "repoonse_2 = \" Sure! Here's the answer to your question:\\n\\nThe typical color of a llama is brown. However, some llamas can also be found in other colors such as white, gray, and even spotted or patchy patterns. So, the colors that are not typical for a llama are white, gray, and any other non-brown colors.\"\n",
        "prompt_3 = \"Summarize the last answer with a few words.\"\n",
        "\n",
        "prompts = [prompt_1, prompt_2, prompt_3]\n",
        "responses = [response_1, response_2]\n",
        "\n",
        "response_3 = llama(get_prompt_chat(prompts, responses), False)\n",
        "print(response_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI3eK4GJbNJw",
        "outputId": "619e83a0-be5d-4932-fda6-c50f47704cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: <s>[INST] The typical color of a llama is what? Answer in a single word. [/INST]\n",
            "Brown\n",
            " </s><s>[INST] \n",
            "What colors are not typical?\n",
            " [/INST]\n",
            " While the typical color of a llama is brown, there are some colors that are not typical or less common, including:\n",
            "\n",
            "* White\n",
            "* Gray\n",
            "* Black\n",
            "* Cream\n",
            "* Roan (a mix of white and brown)\n",
            "* Palomino (a light golden color)\n",
            "* Dun (a mix of red and white)\n",
            "\n",
            "These colors are less common in llamas than the typical brown color, but they can still be found in some breeds and individuals.\n",
            " </s><s>[INST] \n",
            "Summarize the last answer with a few words.\n",
            " [/INST]\n",
            "\n",
            "\n",
            " Less common llama colors: white, gray, black, cream, roan, palomino, dun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Try it out for yourself**"
      ],
      "metadata": {
        "id": "4QL5Z02CkwmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the average lifespan of a Llama? Answer the question in a few words.\"\n",
        "print(llama(prompt))"
      ],
      "metadata": {
        "id": "SsMfRjFDNrep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30eb1b40-8d5e-4112-b1fb-29dbd318b1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [INST]What is the average lifespan of a Llama? Answer the question in a few words.[/INST]\n",
            "\n",
            "\n",
            " The average lifespan of a llama is 15-20 years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without previous context\n",
        "prompt = \"What animal family are they?\"\n",
        "print(llama(prompt))"
      ],
      "metadata": {
        "id": "BMtxjQtpNstX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7de189-742b-4dee-f185-55b29d23bcc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [INST]What animal family are they?[/INST]\n",
            "\n",
            "\n",
            " The animals that are commonly referred to as \"pets\" are members of various animal families. Here are some examples:\n",
            "\n",
            "1. Dogs (Canis lupus familiaris) - They are part of the Canidae family.\n",
            "2. Cats (Felis catus) - They are part of the Felidae family.\n",
            "3. Birds (Aves) - There are many different species of birds that are kept as pets, including parrots, canaries, and finches.\n",
            "4. Fish (Actinopterygii) - Many people keep fish as pets, including goldfish, betta fish, and tropical fish.\n",
            "5. Reptiles (Testudines, Crocodilia, and Squamata) - Some popular reptile pets include turtles, bearded dragons, and geckos.\n",
            "6. Amphibians (Anura) - Frogs and toads are also kept as pets.\n",
            "7. Small mammals ( rodents, rabbits, guinea pigs) - These animals are often kept as pets, especially by children.\n",
            "8. Hedgehogs (Erinaceus europaeus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of multi-turn chat, with storing previous context\n",
        "prompt_chat = \"\"\"\n",
        "User: What is the average lifespan of a Llama?\n",
        "Assistant: The average lifespan of a llama is 15 to 25 years.\n",
        "User: What animal family are they?\n",
        "\n",
        "Answer the question in a few words.\n",
        "\"\"\"\n",
        "print(llama(prompt_chat))"
      ],
      "metadata": {
        "id": "GD1jZaiFNvfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bf5a72-0c3c-49a2-8c91-c4f0d7016f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [INST]\n",
            "User: What is the average lifespan of a Llama?\n",
            "Assistant: The average lifespan of a llama is 15 to 25 years.\n",
            "User: What animal family are they?\n",
            "\n",
            "Answer the question in a few words.\n",
            "[/INST]\n",
            "\n",
            "\n",
            " Llamas are members of the camel family (Camelidae).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use the recommended multi-turn chat format\n",
        "prompts = [\"What is the average lifespan of a Llama?\", \"What animal family are they?\\n\\nAnswer the question in a few words.\"]\n",
        "responses = [\"The average lifespan of a llama is 15 to 25 years.\"]\n",
        "print(llama(get_prompt_chat(prompts, responses), False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z5IXSitAbyW",
        "outputId": "2cc2b91d-645f-4474-97fc-f4ae5a0da799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: <s>[INST] What is the average lifespan of a Llama? [/INST]\n",
            "The average lifespan of a llama is 15 to 25 years.\n",
            " </s><s>[INST] \n",
            "What animal family are they?\n",
            "\n",
            "Answer the question in a few words.\n",
            " [/INST]\n",
            "\n",
            "\n",
            " Llamas are members of the camel family (Camelidae).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you know how to correctly query or chat with Llama 2, the next step is to understand how to best query or chat with Llama 2, covered in the next lesson."
      ],
      "metadata": {
        "id": "bCxrnIQbmWVc"
      }
    }
  ]
}
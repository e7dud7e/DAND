{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Persons of Interest using Machine Learning\n",
    "\n",
    "### Eddy Shyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of project goal\n",
    "\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "Given information about Enron employees and whether they are persons of interest (POI) in fraud, we use machine learning to predict whether employees are persons of interest or not.  The data is primarily numerical, and either compensation related data (salary, bonus, stock options) or email count data (messages sent to persons of interest, all messages sent, etc). There are 144 persons in the dataset (after removing two non-person records), of which 18 are person of interest and the rest are not.  Some features have 100 or more missing values (deferral_payments, restricted_stock_deferred, director_fees, loan_advances), so we'll focus on features that have fewer missing values.  Compensation and email data can represent latent features, such as how similar a person is to a POI, or how professionally connected one is to a POI.  We will be using 11 features, 8 related to compensation, and 3 features related to email counts.\n",
    "\n",
    "Each record should represent a person.  Using a histogram of salary, I found an outlier named \"TOTAL\", which I removed, since it is not a person.  I also removed \"The Travel Agency in the Park\" because it also does not represent a person.  Outliers that represent persons are kept (for example, Jeff Skilling's salary was an outlier that was kept).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "\n",
    "I chose features that represent compensation and email counts, and excluded features that had more than a threshold of missing values.  When a feature has too many missing values, it can unintentionally associate missing values to a particular class.  When choosing the threshold, I wanted to include features that had fewer missing values than the core features that I wanted to keep (email counts), which was 60 missing values out of 144 total records.  The \"bonus\" feature was close enough to the threshold that I kept it as well.\n",
    "\n",
    "I normalized emails sent to persons of interest by dividing by total number of emails sent; similarly for emails received from POI and emails received that were also sent to a person of interest.  This represents the fraction of each person's emails that were associated with a POI.  The latent feature I think that this represents is how close the professional relationship was with POIs.  I scaled all of the compensation and email count ratios to range from 0 to 1, to accommodate algorithms that calculate distances using features, such as SVM.  I used a random forest regression and also selectKBest algorithm to rank features by importance.  Although the orders varied between the two methods, compensation was ranked as more important that the email count ratios.  I take a subset of features, as I will use Principle Component Analysis (PCA) to reduce the dimensions of the features.\n",
    "\n",
    "I compared the final model with and without derived email features.  I found that removing email features and relying only on the compensation data gives us a higher precision but lower recall, and slightly lower F1 score.  One interpretation is that including both compensation and email features help us cast a wider net, which improves recall by increasing the number of true positive predictions, but could hurt precision by predicting more false positives.  Conversely, by excluding email features, we cast a smaller net, and make fewer positive predictions, improving precision but hurting recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm selection\n",
    "\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "I chose Adaboost, with SVM as the base estimator, and also used PCA to reduce the feature dimensions from 11 to 5.  The F1 score, which is higher when there are more true positives and fewer false classifications (false positive, false negative), is slightly above 0.3 for most tests with varying parameters, and as high as 0.34.  At the same time, both precision (true positive divided by all predicted positives) and recall (true positives divided by all actual positives) are above 0.30.  \n",
    "\n",
    "I first tried non-ensemble estimators with and without PCA, including Naive Bayes, Support Vector Machines (SVM), and Decision Trees.  By varying the number of components used, I generally saw that, of 11 features, using between 5 and 9 dimensions yielded higher precision, recall and F1 scores.  Best F1 scores were .27 (trees), .30 (Bayes) .36 (SVM using grid search to optimize parameters).  However, when varying parameters and number of PCA components, F1 scores for any non-ensemble algorithm could dip below 0.30, and often one of either precision or recall would be below 0.30 despite an F1 above 0.30.\n",
    "\n",
    "Next, I tried Adaboost with Bayes, SVM, trees, and logistic regression as base estimators. Adaboost with Bayes did worse than Bayes alone (F1 at most 0.23).  Adaboost with SVM did better than SVM alone in that all three of F1, precision, and recall were above 0.30 for a variety of PCA components, variety of base estimator iterations (each iteration puts more weight on incorrectly classified observations).  Adaboost with decision trees had its highest F1 at .33, but recall was consistently below 0.30.  I also tried a random forest (an average of multiple decision trees). It had its highest F1 of .32 but recall was consistently below 0.30.  I tried Adaboost with logistic regression as the base estimator, and using grid search to vary parameters, got an F1 of .35, precision of .83, but recall of 0.22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]\n",
    "\n",
    "An algorithm's parameters are constants that can be altered to change the bias and variance of the model.  For instance, the SVM's C \"penalty\" can be increased to penalize mis-classifying each data point.  A higher penalty would fit the training data more accurately, but also increase the variance of the model (a different training set would likely change the model significantly).  The SVM's gamma can be increased to reduce the influence of a data point in determining the classification of other points that are far away; this increases the model's bias, so that a different training set is not likely to result in a different model.\n",
    "\n",
    "When using PCA, I varied the number of components used, as well as tested on the original features without PCA.  For Adaboost, I varied the number of estimator iterations.  For SVM, I varied the C (penalty) and gamma.  For decision trees, I varied the minimum number of samples to allow a split.  For random forest, I varied the number of trees.  I used for loops to vary the parameters so that I could run the test and compare validation scores for each combination of parameter values.  I also used Grid Search to see how the model performs on validation testing.  If I understand Grid Search correctly, it tunes the parameters each time it fits to a training set.  So when testing validation uses cross-validation and fits to multiple training sets, the tuned parameters can change each time.  That is why I wanted to use for loops to fix a combination of parameter values for cross validation, to see how the test scores compare for those parameter values.\n",
    "\n",
    "\n",
    "#### Reference:\n",
    "https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]\n",
    "\n",
    "Validation gives us a numerical measure of how well the model would perform when using inputs that were not used to train the model.  In other words, validation lets us state how well the model might do when it is being used to make new predictions.  Validation must use test data that is not part of the training data.  If the same data is used to train and validate a model, then the parameters that give the best validation score will also cause the model to overfit the data, resulting in a high variance model.  Another requirement is for the training and test data to be chosen randomly, so that both sets are representative of the whole data set.  If training and test data are not representative of the whole set, then the model will perform poorly when validated against the test data. For example, if all training data are of POI and all test data are of non-POIs, then the model will make poor predictions when faced with the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "The precision score measures what fraction of all positive predictions are correct ( true positives divided by all predictions of the positive class).  The precision can be 0 at worst and 1 at best.  Using Adaboost with SVM as a base estimator, I got a precision of 0.36.\n",
    "\n",
    "The recall score measures what fraction of all actual positives are correctly discovered ( true positives divided by all actual positives).  The recall can be 0 at worst and 1 at best.  The tuned adaboost had a recall of 0.36.\n",
    "\n",
    "An F1 score equally weighs the precision and recall ( true positives are in the numerator, and false negatives and false positives are in the denominator).  F1 can be 0 at worst and 1 at best.  A model with more false predictions than another model, given the same number of true positives, will have a lower F1 score.  The adaboost had an F1 of 0.36.\n",
    "\n",
    "I used the cross validation method provided, with 100 folds instead of 1,000 to compare the F1, precision, and recall scores.  If I run the validation method with 1,000 folds, the final Adaboost with SVM classifier still has F1, precision and recall at 0.30 or above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: steps taken to select features and choose algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from time import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sys.path.append(\"../tools/\")\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_l = defaultdict(list)\n",
    "for name, content in data_dict.iteritems():    \n",
    "    feature_l['name'].append(name)\n",
    "    for feature, value in content.iteritems():\n",
    "        feature_l[feature].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to_messages',\n",
       " 'deferral_payments',\n",
       " 'expenses',\n",
       " 'poi',\n",
       " 'long_term_incentive',\n",
       " 'email_address',\n",
       " 'from_poi_to_this_person',\n",
       " 'deferred_income',\n",
       " 'restricted_stock_deferred',\n",
       " 'shared_receipt_with_poi',\n",
       " 'loan_advances',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'director_fees',\n",
       " 'bonus',\n",
       " 'total_stock_value',\n",
       " 'from_this_person_to_poi',\n",
       " 'restricted_stock',\n",
       " 'salary',\n",
       " 'name',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options']"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_l.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['METTS MARK',\n",
       " 'BAXTER JOHN C',\n",
       " 'ELLIOTT STEVEN',\n",
       " 'CORDES WILLIAM R',\n",
       " 'HANNON KEVIN P',\n",
       " 'MORDAUNT KRISTINA M',\n",
       " 'MEYER ROCKFORD G',\n",
       " 'MCMAHON JEFFREY',\n",
       " 'HORTON STANLEY C',\n",
       " 'PIPER GREGORY F',\n",
       " 'HUMPHREY GENE E',\n",
       " 'UMANOFF ADAM S',\n",
       " 'BLACHMAN JEREMY M',\n",
       " 'SUNDE MARTIN',\n",
       " 'GIBBS DANA R',\n",
       " 'LOWRY CHARLES P',\n",
       " 'COLWELL WESLEY',\n",
       " 'MULLER MARK S',\n",
       " 'JACKSON CHARLENE R',\n",
       " 'WESTFAHL RICHARD K',\n",
       " 'WALTERS GARETH W',\n",
       " 'WALLS JR ROBERT H',\n",
       " 'KITCHEN LOUISE',\n",
       " 'CHAN RONNIE',\n",
       " 'BELFER ROBERT',\n",
       " 'SHANKMAN JEFFREY A',\n",
       " 'WODRASKA JOHN',\n",
       " 'BERGSIEKER RICHARD P',\n",
       " 'URQUHART JOHN A',\n",
       " 'BIBI PHILIPPE A',\n",
       " 'RIEKER PAULA H',\n",
       " 'WHALEY DAVID A',\n",
       " 'BECK SALLY W',\n",
       " 'HAUG DAVID L',\n",
       " 'ECHOLS JOHN B',\n",
       " 'MENDELSOHN JOHN',\n",
       " 'HICKERSON GARY J',\n",
       " 'CLINE KENNETH W',\n",
       " 'LEWIS RICHARD',\n",
       " 'HAYES ROBERT E',\n",
       " 'MCCARTY DANNY J',\n",
       " 'KOPPER MICHAEL J',\n",
       " 'LEFF DANIEL P',\n",
       " 'LAVORATO JOHN J',\n",
       " 'BERBERIAN DAVID',\n",
       " 'DETMERING TIMOTHY J',\n",
       " 'WAKEHAM JOHN',\n",
       " 'POWERS WILLIAM',\n",
       " 'GOLD JOSEPH',\n",
       " 'BANNANTINE JAMES M',\n",
       " 'DUNCAN JOHN H',\n",
       " 'SHAPIRO RICHARD S',\n",
       " 'SHERRIFF JOHN R',\n",
       " 'SHELBY REX',\n",
       " 'LEMAISTRE CHARLES',\n",
       " 'DEFFNER JOSEPH M',\n",
       " 'KISHKILL JOSEPH G',\n",
       " 'WHALLEY LAWRENCE G',\n",
       " 'MCCONNELL MICHAEL S',\n",
       " 'PIRO JIM',\n",
       " 'DELAINEY DAVID W',\n",
       " 'SULLIVAN-SHAKLOVITZ COLLEEN',\n",
       " 'WROBEL BRUCE',\n",
       " 'LINDHOLM TOD A',\n",
       " 'MEYER JEROME J',\n",
       " 'LAY KENNETH L',\n",
       " 'BUTTS ROBERT H',\n",
       " 'OLSON CINDY K',\n",
       " 'MCDONALD REBECCA',\n",
       " 'CUMBERLAND MICHAEL S',\n",
       " 'GAHN ROBERT S',\n",
       " 'MCCLELLAN GEORGE',\n",
       " 'HERMANN ROBERT J',\n",
       " 'SCRIMSHAW MATTHEW',\n",
       " 'GATHMANN WILLIAM D',\n",
       " 'HAEDICKE MARK E',\n",
       " 'BOWEN JR RAYMOND M',\n",
       " 'GILLIS JOHN',\n",
       " 'FITZGERALD JAY L',\n",
       " 'MORAN MICHAEL P',\n",
       " 'REDMOND BRIAN L',\n",
       " 'BAZELIDES PHILIP J',\n",
       " 'BELDEN TIMOTHY N',\n",
       " 'DURAN WILLIAM D',\n",
       " 'THORN TERENCE H',\n",
       " 'FASTOW ANDREW S',\n",
       " 'FOY JOE',\n",
       " 'CALGER CHRISTOPHER F',\n",
       " 'RICE KENNETH D',\n",
       " 'KAMINSKI WINCENTY J',\n",
       " 'LOCKHART EUGENE E',\n",
       " 'COX DAVID',\n",
       " 'OVERDYKE JR JERE C',\n",
       " 'PEREIRA PAULO V. FERRAZ',\n",
       " 'STABLER FRANK',\n",
       " 'SKILLING JEFFREY K',\n",
       " 'BLAKE JR. NORMAN P',\n",
       " 'SHERRICK JEFFREY B',\n",
       " 'PRENTICE JAMES',\n",
       " 'GRAY RODNEY',\n",
       " 'PICKERING MARK R',\n",
       " 'THE TRAVEL AGENCY IN THE PARK',\n",
       " 'NOLES JAMES L',\n",
       " 'KEAN STEVEN J',\n",
       " 'TOTAL',\n",
       " 'FOWLER PEGGY',\n",
       " 'WASAFF GEORGE',\n",
       " 'WHITE JR THOMAS E',\n",
       " 'CHRISTODOULOU DIOMEDES',\n",
       " 'ALLEN PHILLIP K',\n",
       " 'SHARP VICTORIA T',\n",
       " 'JAEDICKE ROBERT',\n",
       " 'WINOKUR JR. HERBERT S',\n",
       " 'BROWN MICHAEL',\n",
       " 'BADUM JAMES P',\n",
       " 'HUGHES JAMES A',\n",
       " 'REYNOLDS LAWRENCE',\n",
       " 'DIMICHELE RICHARD G',\n",
       " 'BHATNAGAR SANJAY',\n",
       " 'CARTER REBECCA C',\n",
       " 'BUCHANAN HAROLD G',\n",
       " 'YEAP SOON',\n",
       " 'MURRAY JULIA H',\n",
       " 'GARLAND C KEVIN',\n",
       " 'DODSON KEITH',\n",
       " 'YEAGER F SCOTT',\n",
       " 'HIRKO JOSEPH',\n",
       " 'DIETRICH JANET R',\n",
       " 'DERRICK JR. JAMES V',\n",
       " 'FREVERT MARK A',\n",
       " 'PAI LOU L',\n",
       " 'BAY FRANKLIN R',\n",
       " 'HAYSLETT RODERICK J',\n",
       " 'FUGH JOHN L',\n",
       " 'FALLON JAMES B',\n",
       " 'KOENIG MARK E',\n",
       " 'SAVAGE FRANK',\n",
       " 'IZZO LAWRENCE L',\n",
       " 'TILNEY ELIZABETH A',\n",
       " 'MARTIN AMANDA K',\n",
       " 'BUY RICHARD B',\n",
       " 'GRAMM WENDY L',\n",
       " 'CAUSEY RICHARD A',\n",
       " 'TAYLOR MITCHELL S',\n",
       " 'DONAHUE JR JEFFREY M',\n",
       " 'GLISAN JR BEN F']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_l['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the names, 'The Travel Agency in the Park' is not a person, so we'll remove this.  We are trying to identify persons of interest, so we only want to train and test on person data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert dict to a data frame to describe data and plot it\n",
    "data_df = pd.DataFrame(feature_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove NaN from each col and plot it for outliers\n",
    "salary = data_df['salary']\n",
    "salary_c = salary[salary.apply(lambda x: not math.isnan(float(x)))]\n",
    "salary_c = pd.DataFrame(salary_c.apply(lambda x: float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.621943e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.716369e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.770000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.118160e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.599960e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.121170e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.670423e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             salary\n",
       "count  9.500000e+01\n",
       "mean   5.621943e+05\n",
       "std    2.716369e+06\n",
       "min    4.770000e+02\n",
       "25%    2.118160e+05\n",
       "50%    2.599960e+05\n",
       "75%    3.121170e+05\n",
       "max    2.670423e+07"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_c.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram shows an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwXGRuC5kzsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f8b1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "salary_c.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26704229.0"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_c.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104    TOTAL\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['salary']==salary_c.max()[0]]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the 'TOTAL' record from data_dict and the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_dict.pop('TOTAL')\n",
    "data_df = data_df[data_df['name'] != 'TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = data_df[data_df['name'] != 'THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi = data_df['poi']\n",
    "poi = poi.apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove max from from salary_c and check again\n",
    "salary_c = salary_c[ salary_c['salary'] != salary_c['salary'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEKCAYAAADw2zkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH+9waK7NIicwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fa910d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "salary_c.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111258.0"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_c['salary'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95    SKILLING JEFFREY K\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[ data_df['salary'] == salary_c['salary'].max() ]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this second 'outlier' is a person, we'll keep this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for other outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expenses = data_df['expenses']\n",
    "expenses_c = expenses[expenses.apply(lambda x: not math.isnan(float(x)))]\n",
    "expenses_c = pd.DataFrame(expenses_c.apply(lambda x: float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQhkAklQoA0CSCvX/HAuAy9+EoVwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f0c4f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expenses_c.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54192.010638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46108.377454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22479.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46547.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78408.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>228763.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            expenses\n",
       "count      94.000000\n",
       "mean    54192.010638\n",
       "std     46108.377454\n",
       "min       148.000000\n",
       "25%     22479.000000\n",
       "50%     46547.500000\n",
       "75%     78408.500000\n",
       "max    228763.000000"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expenses_c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71    MCCLELLAN GEORGE\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[ data_df['expenses'] == expenses_c['expenses'].max() ]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max expense is from a person; we'll keep this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nan_count = defaultdict(list)\n",
    "for k, v in feature_l.iteritems():\n",
    "    num_nan = sum([1 for e in v if e == 'NaN'] )\n",
    "    nan_count['feature'].append(k)\n",
    "    nan_count['nan_count'].append(num_nan)\n",
    "    \n",
    "nan_count = pd.DataFrame(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_stock_value</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>total_payments</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>email_address</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>expenses</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>salary</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>other</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>from_this_person_to_poi</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to_messages</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shared_receipt_with_poi</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>from_poi_to_this_person</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from_messages</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bonus</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  nan_count\n",
       "19                       name          0\n",
       "3                         poi          0\n",
       "15          total_stock_value         20\n",
       "20             total_payments         21\n",
       "5               email_address         35\n",
       "17           restricted_stock         36\n",
       "21    exercised_stock_options         44\n",
       "2                    expenses         51\n",
       "18                     salary         51\n",
       "12                      other         53\n",
       "16    from_this_person_to_poi         60\n",
       "0                 to_messages         60\n",
       "9     shared_receipt_with_poi         60\n",
       "6     from_poi_to_this_person         60\n",
       "11              from_messages         60\n",
       "14                      bonus         64\n",
       "4         long_term_incentive         80\n",
       "7             deferred_income         97\n",
       "1           deferral_payments        107\n",
       "8   restricted_stock_deferred        128\n",
       "13              director_fees        129\n",
       "10              loan_advances        142"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_count.sort_values('nan_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer features that have more data as opposed to missing data.  Since I plan to use the email data, which have 60 NaNs per feature at most, I'll use that as the cut-off point.  Since 'bonus' is close to that cut-off at 64 NaNs, I'll also include 'bonus' and all features with fewer NaNs.  I'll exclude 'email address', since it is not numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### feature_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "#The features are either related to compensation/expenses or to emails\n",
    "feature_list = ['poi',\n",
    "                 'salary',\n",
    "                 'total_stock_value',\n",
    "                 'total_payments',\n",
    "                 'restricted_stock',\n",
    "                 'exercised_stock_options',\n",
    "                 'other',\n",
    "                 'bonus',\n",
    "                 'expenses',\n",
    "                 'to_messages',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'shared_receipt_with_poi'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two members appear not to be persons, so I'll remove these from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Task 2 Remove outliers\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 97343619,\n",
       " 'deferral_payments': 32083396,\n",
       " 'deferred_income': -27992891,\n",
       " 'director_fees': 1398517,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'expenses': 5235198,\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 83925000,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'other': 42667589,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 130322299,\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'salary': 26704229,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 309886585,\n",
       " 'total_stock_value': 434509511}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop('TOTAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new features\n",
    "\n",
    "I can scale the compensation data to be between 0 and 1.  This is helpful if using SVM or K-means which calculate a distance based on more than one dimension.\n",
    "\n",
    "For emails, I can get a ratio of poi emails received divided by all emails received, and similarly for other poi_emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scale_feature(data_dict, feature, feature_scaled):\n",
    "    feature_l = [v[feature] for v in data_dict.values() if v[feature] != 'NaN']\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.array(feature_l).reshape(len(feature_l),1))\n",
    "    \n",
    "    for name, data in data_dict.iteritems():\n",
    "        if data[feature] == 'NaN':\n",
    "            data[feature_scaled] = 'NaN'\n",
    "        else:\n",
    "            data[feature_scaled] = scaler.transform(np.array([[data[feature]]]))[0][0]\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ratio(data_dict, numerator, denominator, ratio):\n",
    "    for k, v in data_dict.iteritems():\n",
    "        n = v[numerator]\n",
    "        d = v[denominator]\n",
    "        if n == 'NaN' or d == 'NaN' or d == 0:\n",
    "            data_dict[k][ratio] = 'NaN'\n",
    "        else:\n",
    "            data_dict[k][ratio] = float(n) / float(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first compute email ratios\n",
    "ratios_to_compute = [('from_this_person_to_poi', 'from_messages', 'to_poi_ratio'),\n",
    "                     ('from_poi_to_this_person', 'to_messages', 'from_poi_ratio'),\n",
    "                     ('shared_receipt_with_poi', 'to_messages', 'shared_poi_ratio')\n",
    "                    ]\n",
    "\n",
    "for numerator, denominator, ratio in ratios_to_compute:\n",
    "    compute_ratio(data_dict, numerator, denominator, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then scale compensation, expense, and email ratios\n",
    "feature_to_scale = ['salary',\n",
    "                    'total_stock_value',\n",
    "                     'total_payments',\n",
    "                     'restricted_stock',\n",
    "                     'exercised_stock_options',\n",
    "                     'other',\n",
    "                     'bonus',\n",
    "                     'expenses',\n",
    "                     'to_poi_ratio',\n",
    "                     'from_poi_ratio',\n",
    "                     'shared_poi_ratio'\n",
    "                    ]\n",
    "for feature in feature_to_scale:\n",
    "    data_dict = scale_feature(data_dict, feature, feature + '_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank features by importance\n",
    "\n",
    "I'll try both Random Forest and selectkbest method to rank features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.17, 'expenses_scaled'), (0.15, 'exercised_stock_options_scaled'), (0.12, 'restricted_stock_scaled'), (0.11, 'other_scaled'), (0.11, 'bonus_scaled'), (0.1, 'salary_scaled'), (0.08, 'from_poi_ratio_scaled'), (0.06, 'total_payments_scaled'), (0.04, 'total_stock_value_scaled'), (0.03, 'to_poi_ratio_scaled'), (0.02, 'poi')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(features, labels)\n",
    "print sorted (zip (map(lambda x: round(x,2),clf.feature_importances_) , feature_list), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank feature using selectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24.0, 'salary_scaled'), (24.0, 'restricted_stock_scaled'), (20.0, 'other_scaled'), (18.0, 'poi'), (16.0, 'expenses_scaled'), (11.0, 'total_payments_scaled'), (9.0, 'total_stock_value_scaled'), (9.0, 'from_poi_ratio_scaled'), (6.0, 'bonus_scaled'), (4.0, 'exercised_stock_options_scaled'), (3.0, 'to_poi_ratio_scaled')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "clf = SelectKBest(k=11)\n",
    "clf.fit(features, labels)\n",
    "print sorted (zip (map(lambda x: round(x,0),clf.scores_), feature_list), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes with and without PCA\n",
    "\n",
    "If I vary the components for each test, I find the best F1 score of 0.30151 at 9 components.  Whereas if I run grid search, it gives 4 as the best number of components; but this gives me lower precision and recall when I run it through the test.  So will try using and not using GridSearch when trying to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PCA with None components\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83867\tPrecision: 0.33465\tRecall: 0.21250\tF1: 0.25994\tF2: 0.22923\n",
      "\tTotal predictions: 15000\tTrue positives:  425\tFalse positives:  845\tFalse negatives: 1575\tTrue negatives: 12155\n",
      "\n",
      "Using PCA with 3 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.84833\tPrecision: 0.35167\tRecall: 0.16300\tF1: 0.22275\tF2: 0.18259\n",
      "\tTotal predictions: 15000\tTrue positives:  326\tFalse positives:  601\tFalse negatives: 1674\tTrue negatives: 12399\n",
      "\n",
      "Using PCA with 4 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.84440\tPrecision: 0.32384\tRecall: 0.15350\tF1: 0.20828\tF2: 0.17155\n",
      "\tTotal predictions: 15000\tTrue positives:  307\tFalse positives:  641\tFalse negatives: 1693\tTrue negatives: 12359\n",
      "\n",
      "Using PCA with 5 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.83947\tPrecision: 0.32322\tRecall: 0.18650\tF1: 0.23653\tF2: 0.20374\n",
      "\tTotal predictions: 15000\tTrue positives:  373\tFalse positives:  781\tFalse negatives: 1627\tTrue negatives: 12219\n",
      "\n",
      "Using PCA with 6 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=6, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.82007\tPrecision: 0.27082\tRecall: 0.20650\tF1: 0.23433\tF2: 0.21680\n",
      "\tTotal predictions: 15000\tTrue positives:  413\tFalse positives: 1112\tFalse negatives: 1587\tTrue negatives: 11888\n",
      "\n",
      "Using PCA with 7 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.82240\tPrecision: 0.31115\tRecall: 0.27350\tF1: 0.29111\tF2: 0.28028\n",
      "\tTotal predictions: 15000\tTrue positives:  547\tFalse positives: 1211\tFalse negatives: 1453\tTrue negatives: 11789\n",
      "\n",
      "Using PCA with 8 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.82193\tPrecision: 0.31474\tRecall: 0.28500\tF1: 0.29913\tF2: 0.29049\n",
      "\tTotal predictions: 15000\tTrue positives:  570\tFalse positives: 1241\tFalse negatives: 1430\tTrue negatives: 11759\n",
      "\n",
      "Using PCA with 9 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.81220\tPrecision: 0.29907\tRecall: 0.30400\tF1: 0.30151\tF2: 0.30300\n",
      "\tTotal predictions: 15000\tTrue positives:  608\tFalse positives: 1425\tFalse negatives: 1392\tTrue negatives: 11575\n",
      "\n",
      "Using PCA with 10 components\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gaussiannb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.80087\tPrecision: 0.25139\tRecall: 0.24950\tF1: 0.25044\tF2: 0.24987\n",
      "\tTotal predictions: 15000\tTrue positives:  499\tFalse positives: 1486\tFalse negatives: 1501\tTrue negatives: 11514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "for n_components in [None,3,4,5,6,7,8,9,10]:\n",
    "    \n",
    "    if n_components == None:\n",
    "        print \"Naive Bayes without PCA\"\n",
    "        gnb = GaussianNB()\n",
    "        test_classifier(clf=gnb, dataset=my_dataset, feature_list=feature_list, folds=1000)\n",
    "    else:\n",
    "        print \"Naive Bayes with PCA using {} components\".format(n_components)\n",
    "        pca = PCA(n_components=n_components)\n",
    "        gnb = GaussianNB()\n",
    "        pipe = make_pipeline(pca, gnb)\n",
    "        test_classifier(clf=pipe, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried using GridSearch to use either no PCA or PCA with varying components, but the results have a lower precision/accuracy than when I use a loop to vary the use of PCA.  The F1 score is 0.27257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gnb', GaussianNB(priors=None))]),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'pca': [None, PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False), PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n",
      "\tAccuracy: 0.83027\tPrecision: 0.31800\tRecall: 0.23850\tF1: 0.27257\tF2: 0.25105\n",
      "\tTotal predictions: 15000\tTrue positives:  477\tFalse positives: 1023\tFalse negatives: 1523\tTrue negatives: 11977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pipeline PCA and Naive Bayes and GridSearch\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "estimators = [('pca', pca), \n",
    "              ('gnb', gnb)]\n",
    "pipe = Pipeline(estimators)\n",
    "params = dict(pca = [None,\n",
    "                     PCA(n_components=8),\n",
    "                     PCA(n_components=9)\n",
    "                    ])\n",
    "clf = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine with and without PCA\n",
    "\n",
    "The best F1 score I see is .28289 for when I don't use PCA, C is 10000.0, gamma is either 0.0005 or 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines with PCA with different components\n",
      "SVC without PCA, C 5000.0 gamma 0.0005\n",
      "SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.88400\tPrecision: 0.86111\tRecall: 0.15500\tF1: 0.26271\tF2: 0.18541\n",
      "\tTotal predictions: 1500\tTrue positives:   31\tFalse positives:    5\tFalse negatives:  169\tTrue negatives: 1295\n",
      "\n",
      "SVC without PCA, C 5000.0 gamma 0.001\n",
      "SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.88733\tPrecision: 0.89744\tRecall: 0.17500\tF1: 0.29289\tF2: 0.20858\n",
      "\tTotal predictions: 1500\tTrue positives:   35\tFalse positives:    4\tFalse negatives:  165\tTrue negatives: 1296\n",
      "\n",
      "SVC without PCA, C 5000.0 gamma 0.005\n",
      "SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.87933\tPrecision: 0.77143\tRecall: 0.13500\tF1: 0.22979\tF2: 0.16168\n",
      "\tTotal predictions: 1500\tTrue positives:   27\tFalse positives:    8\tFalse negatives:  173\tTrue negatives: 1292\n",
      "\n",
      "SVC without PCA, C 10000.0 gamma 0.0005\n",
      "SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.88733\tPrecision: 0.89744\tRecall: 0.17500\tF1: 0.29289\tF2: 0.20858\n",
      "\tTotal predictions: 1500\tTrue positives:   35\tFalse positives:    4\tFalse negatives:  165\tTrue negatives: 1296\n",
      "\n",
      "SVC without PCA, C 10000.0 gamma 0.001\n",
      "SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.88333\tPrecision: 0.83784\tRecall: 0.15500\tF1: 0.26160\tF2: 0.18519\n",
      "\tTotal predictions: 1500\tTrue positives:   31\tFalse positives:    6\tFalse negatives:  169\tTrue negatives: 1294\n",
      "\n",
      "SVC without PCA, C 10000.0 gamma 0.005\n",
      "SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.87667\tPrecision: 0.71429\tRecall: 0.12500\tF1: 0.21277\tF2: 0.14970\n",
      "\tTotal predictions: 1500\tTrue positives:   25\tFalse positives:   10\tFalse negatives:  175\tTrue negatives: 1290\n",
      "\n",
      "SVC without PCA, C 50000.0 gamma 0.0005\n",
      "SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.88000\tPrecision: 0.76316\tRecall: 0.14500\tF1: 0.24370\tF2: 0.17303\n",
      "\tTotal predictions: 1500\tTrue positives:   29\tFalse positives:    9\tFalse negatives:  171\tTrue negatives: 1291\n",
      "\n",
      "SVC without PCA, C 50000.0 gamma 0.001\n",
      "SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.87800\tPrecision: 0.71795\tRecall: 0.14000\tF1: 0.23431\tF2: 0.16687\n",
      "\tTotal predictions: 1500\tTrue positives:   28\tFalse positives:   11\tFalse negatives:  172\tTrue negatives: 1289\n",
      "\n",
      "SVC without PCA, C 50000.0 gamma 0.005\n",
      "SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.86200\tPrecision: 0.42222\tRecall: 0.09500\tF1: 0.15510\tF2: 0.11243\n",
      "\tTotal predictions: 1500\tTrue positives:   19\tFalse positives:   26\tFalse negatives:  181\tTrue negatives: 1274\n",
      "\n",
      "SVC with PCA & 5 components, C 5000.0, gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86800\tPrecision: 0.75000\tRecall: 0.01500\tF1: 0.02941\tF2: 0.01866\n",
      "\tTotal predictions: 1500\tTrue positives:    3\tFalse positives:    1\tFalse negatives:  197\tTrue negatives: 1299\n",
      "\n",
      "SVC with PCA & 5 components, C 5000.0, gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86800\tPrecision: 0.66667\tRecall: 0.02000\tF1: 0.03883\tF2: 0.02481\n",
      "\tTotal predictions: 1500\tTrue positives:    4\tFalse positives:    2\tFalse negatives:  196\tTrue negatives: 1298\n",
      "\n",
      "SVC with PCA & 5 components, C 5000.0, gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86667\tPrecision: 0.50000\tRecall: 0.02000\tF1: 0.03846\tF2: 0.02475\n",
      "\tTotal predictions: 1500\tTrue positives:    4\tFalse positives:    4\tFalse negatives:  196\tTrue negatives: 1296\n",
      "\n",
      "SVC with PCA & 5 components, C 10000.0, gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86800\tPrecision: 0.66667\tRecall: 0.02000\tF1: 0.03883\tF2: 0.02481\n",
      "\tTotal predictions: 1500\tTrue positives:    4\tFalse positives:    2\tFalse negatives:  196\tTrue negatives: 1298\n",
      "\n",
      "SVC with PCA & 5 components, C 10000.0, gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86733\tPrecision: 0.57143\tRecall: 0.02000\tF1: 0.03865\tF2: 0.02478\n",
      "\tTotal predictions: 1500\tTrue positives:    4\tFalse positives:    3\tFalse negatives:  196\tTrue negatives: 1297\n",
      "\n",
      "SVC with PCA & 5 components, C 10000.0, gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86800\tPrecision: 0.56250\tRecall: 0.04500\tF1: 0.08333\tF2: 0.05515\n",
      "\tTotal predictions: 1500\tTrue positives:    9\tFalse positives:    7\tFalse negatives:  191\tTrue negatives: 1293\n",
      "\n",
      "SVC with PCA & 5 components, C 50000.0, gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86667\tPrecision: 0.50000\tRecall: 0.02000\tF1: 0.03846\tF2: 0.02475\n",
      "\tTotal predictions: 1500\tTrue positives:    4\tFalse positives:    4\tFalse negatives:  196\tTrue negatives: 1296\n",
      "\n",
      "SVC with PCA & 5 components, C 50000.0, gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86600\tPrecision: 0.44444\tRecall: 0.02000\tF1: 0.03828\tF2: 0.02472\n",
      "\tTotal predictions: 1500\tTrue positives:    4\tFalse positives:    5\tFalse negatives:  196\tTrue negatives: 1295\n",
      "\n",
      "SVC with PCA & 5 components, C 50000.0, gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86400\tPrecision: 0.41667\tRecall: 0.05000\tF1: 0.08929\tF2: 0.06068\n",
      "\tTotal predictions: 1500\tTrue positives:   10\tFalse positives:   14\tFalse negatives:  190\tTrue negatives: 1286\n",
      "\n",
      "SVC with PCA & 7 components, C 5000.0, gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87000\tPrecision: 0.69231\tRecall: 0.04500\tF1: 0.08451\tF2: 0.05535\n",
      "\tTotal predictions: 1500\tTrue positives:    9\tFalse positives:    4\tFalse negatives:  191\tTrue negatives: 1296\n",
      "\n",
      "SVC with PCA & 7 components, C 5000.0, gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87000\tPrecision: 0.66667\tRecall: 0.05000\tF1: 0.09302\tF2: 0.06135\n",
      "\tTotal predictions: 1500\tTrue positives:   10\tFalse positives:    5\tFalse negatives:  190\tTrue negatives: 1295\n",
      "\n",
      "SVC with PCA & 7 components, C 5000.0, gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=5000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87133\tPrecision: 0.70588\tRecall: 0.06000\tF1: 0.11060\tF2: 0.07344\n",
      "\tTotal predictions: 1500\tTrue positives:   12\tFalse positives:    5\tFalse negatives:  188\tTrue negatives: 1295\n",
      "\n",
      "SVC with PCA & 7 components, C 10000.0, gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87000\tPrecision: 0.66667\tRecall: 0.05000\tF1: 0.09302\tF2: 0.06135\n",
      "\tTotal predictions: 1500\tTrue positives:   10\tFalse positives:    5\tFalse negatives:  190\tTrue negatives: 1295\n",
      "\n",
      "SVC with PCA & 7 components, C 10000.0, gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87200\tPrecision: 0.72222\tRecall: 0.06500\tF1: 0.11927\tF2: 0.07946\n",
      "\tTotal predictions: 1500\tTrue positives:   13\tFalse positives:    5\tFalse negatives:  187\tTrue negatives: 1295\n",
      "\n",
      "SVC with PCA & 7 components, C 10000.0, gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=10000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87067\tPrecision: 0.66667\tRecall: 0.06000\tF1: 0.11009\tF2: 0.07335\n",
      "\tTotal predictions: 1500\tTrue positives:   12\tFalse positives:    6\tFalse negatives:  188\tTrue negatives: 1294\n",
      "\n",
      "SVC with PCA & 7 components, C 50000.0, gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87200\tPrecision: 0.72222\tRecall: 0.06500\tF1: 0.11927\tF2: 0.07946\n",
      "\tTotal predictions: 1500\tTrue positives:   13\tFalse positives:    5\tFalse negatives:  187\tTrue negatives: 1295\n",
      "\n",
      "SVC with PCA & 7 components, C 50000.0, gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87200\tPrecision: 0.72222\tRecall: 0.06500\tF1: 0.11927\tF2: 0.07946\n",
      "\tTotal predictions: 1500\tTrue positives:   13\tFalse positives:    5\tFalse negatives:  187\tTrue negatives: 1295\n",
      "\n",
      "SVC with PCA & 7 components, C 50000.0, gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86533\tPrecision: 0.46667\tRecall: 0.07000\tF1: 0.12174\tF2: 0.08434\n",
      "\tTotal predictions: 1500\tTrue positives:   14\tFalse positives:   16\tFalse negatives:  186\tTrue negatives: 1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "print \"Support Vector Machines with PCA with different components\"\n",
    "\n",
    "for n_components in [None,5,7]:\n",
    "    for C in [5e3, 1e4, 5e4]:\n",
    "        for gamma in [0.0005, 0.001, 0.005]:\n",
    "\n",
    "            if n_components == None:\n",
    "                print \"SVC without PCA, C {} gamma {}\".format(C,gamma)\n",
    "                clf = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "                test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "            else:\n",
    "                print \"SVC with PCA & {} components, C {}, gamma {}\".format(n_components,\n",
    "                                                                               C, gamma)\n",
    "                pca = PCA(n_components=n_components)\n",
    "                svc = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "                clf = make_pipeline(pca, svc)\n",
    "                test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using GridSearch, each time I run fit, it will look for the best parameter, which may not be the same whenever the training data changes.  So when I pass the grid search to the test method, since it runs fit multiple times as it re-shuffles the data to create mutiple training sets, I'm ending up with higher precision, recall and F1 scores.  \n",
    "\n",
    "The highest F1 score I see is 0.35951, when there are 4 PCA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines with PCA with different components\n",
      "PCA with 4 components, SVC grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.75533\tPrecision: 0.27614\tRecall: 0.51500\tF1: 0.35951\tF2: 0.43905\n",
      "\tTotal predictions: 1500\tTrue positives:  103\tFalse positives:  270\tFalse negatives:   97\tTrue negatives: 1030\n",
      "\n",
      "PCA with 5 components, SVC grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.75733\tPrecision: 0.25152\tRecall: 0.41500\tF1: 0.31321\tF2: 0.36726\n",
      "\tTotal predictions: 1500\tTrue positives:   83\tFalse positives:  247\tFalse negatives:  117\tTrue negatives: 1053\n",
      "\n",
      "PCA with 6 components, SVC grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=6, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.79533\tPrecision: 0.28163\tRecall: 0.34500\tF1: 0.31011\tF2: 0.33014\n",
      "\tTotal predictions: 1500\tTrue positives:   69\tFalse positives:  176\tFalse negatives:  131\tTrue negatives: 1124\n",
      "\n",
      "PCA with 7 components, SVC grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.77933\tPrecision: 0.24111\tRecall: 0.30500\tF1: 0.26932\tF2: 0.28965\n",
      "\tTotal predictions: 1500\tTrue positives:   61\tFalse positives:  192\tFalse negatives:  139\tTrue negatives: 1108\n",
      "\n",
      "PCA with 8 components, SVC grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.78000\tPrecision: 0.22458\tRecall: 0.26500\tF1: 0.24312\tF2: 0.25579\n",
      "\tTotal predictions: 1500\tTrue positives:   53\tFalse positives:  183\tFalse negatives:  147\tTrue negatives: 1117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Support Vector Machines with PCA with different components\"\n",
    "for n_components in [4,5,6,7,8]:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    param_grid = {\n",
    "          'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "          'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "          }\n",
    "    grid_svc = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "    clf = make_pipeline(pca,grid_svc)\n",
    "    print \"PCA with {} components, SVC grid search\".format(n_components)\n",
    "    test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "The highest F1 I see is 0.24157 when PCA is used with 5 components and min samples to split is 20.  But F1 scores are generally low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree min samples to split 20 without PCA\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.81800\tPrecision: 0.22137\tRecall: 0.14500\tF1: 0.17523\tF2: 0.15575\n",
      "\tTotal predictions: 1500\tTrue positives:   29\tFalse positives:  102\tFalse negatives:  171\tTrue negatives: 1198\n",
      "\n",
      "Decision Tree min samples to split 50 without PCA\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.83333\tPrecision: 0.14286\tRecall: 0.05000\tF1: 0.07407\tF2: 0.05747\n",
      "\tTotal predictions: 1500\tTrue positives:   10\tFalse positives:   60\tFalse negatives:  190\tTrue negatives: 1240\n",
      "\n",
      "Decision Tree min samples to split 70 without PCA\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.83333\tPrecision: 0.14286\tRecall: 0.05000\tF1: 0.07407\tF2: 0.05747\n",
      "\tTotal predictions: 1500\tTrue positives:   10\tFalse positives:   60\tFalse negatives:  190\tTrue negatives: 1240\n",
      "\n",
      "PCA 5, Decision Tree min_samples_split 20 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.82000\tPrecision: 0.27564\tRecall: 0.21500\tF1: 0.24157\tF2: 0.22490\n",
      "\tTotal predictions: 1500\tTrue positives:   43\tFalse positives:  113\tFalse negatives:  157\tTrue negatives: 1187\n",
      "\n",
      "PCA 5, Decision Tree min_samples_split 50 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.83600\tPrecision: 0.12903\tRecall: 0.04000\tF1: 0.06107\tF2: 0.04640\n",
      "\tTotal predictions: 1500\tTrue positives:    8\tFalse positives:   54\tFalse negatives:  192\tTrue negatives: 1246\n",
      "\n",
      "PCA 5, Decision Tree min_samples_split 70 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.83600\tPrecision: 0.12903\tRecall: 0.04000\tF1: 0.06107\tF2: 0.04640\n",
      "\tTotal predictions: 1500\tTrue positives:    8\tFalse positives:   54\tFalse negatives:  192\tTrue negatives: 1246\n",
      "\n",
      "PCA 7, Decision Tree min_samples_split 20 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.81000\tPrecision: 0.22222\tRecall: 0.17000\tF1: 0.19263\tF2: 0.17838\n",
      "\tTotal predictions: 1500\tTrue positives:   34\tFalse positives:  119\tFalse negatives:  166\tTrue negatives: 1181\n",
      "\n",
      "PCA 7, Decision Tree min_samples_split 50 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.82867\tPrecision: 0.13924\tRecall: 0.05500\tF1: 0.07885\tF2: 0.06257\n",
      "\tTotal predictions: 1500\tTrue positives:   11\tFalse positives:   68\tFalse negatives:  189\tTrue negatives: 1232\n",
      "\n",
      "PCA 7, Decision Tree min_samples_split 70 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.83133\tPrecision: 0.15584\tRecall: 0.06000\tF1: 0.08664\tF2: 0.06842\n",
      "\tTotal predictions: 1500\tTrue positives:   12\tFalse positives:   65\tFalse negatives:  188\tTrue negatives: 1235\n",
      "\n",
      "PCA 9, Decision Tree min_samples_split 20 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.80067\tPrecision: 0.20359\tRecall: 0.17000\tF1: 0.18529\tF2: 0.17580\n",
      "\tTotal predictions: 1500\tTrue positives:   34\tFalse positives:  133\tFalse negatives:  166\tTrue negatives: 1167\n",
      "\n",
      "PCA 9, Decision Tree min_samples_split 50 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.82400\tPrecision: 0.14444\tRecall: 0.06500\tF1: 0.08966\tF2: 0.07303\n",
      "\tTotal predictions: 1500\tTrue positives:   13\tFalse positives:   77\tFalse negatives:  187\tTrue negatives: 1223\n",
      "\n",
      "PCA 9, Decision Tree min_samples_split 70 PCA\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.82467\tPrecision: 0.14607\tRecall: 0.06500\tF1: 0.08997\tF2: 0.07312\n",
      "\tTotal predictions: 1500\tTrue positives:   13\tFalse positives:   76\tFalse negatives:  187\tTrue negatives: 1224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for n_components in [None,5,7,9]:\n",
    "    for min_samples_split in [20,50,70]:\n",
    "        if n_components == None:\n",
    "            print \"Decision Tree min samples to split {} without PCA\".format(min_samples_split)\n",
    "            clf = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "            test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "            \n",
    "        else:\n",
    "            print \"PCA {}, Decision Tree min_samples_split {} PCA\".format(n_components,\n",
    "                                                                                min_samples_split)\n",
    "            pca = PCA(n_components=n_components)\n",
    "            dtc = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "            clf = make_pipeline(pca, dtc)\n",
    "            test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Decision Tree with GridSearch, accuracy was lower than when I did not use grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree grid search, no PCA\n",
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'min_samples_split': [20, 50, 70]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n",
      "\tAccuracy: 0.82867\tPrecision: 0.16471\tRecall: 0.07000\tF1: 0.09825\tF2: 0.07910\n",
      "\tTotal predictions: 1500\tTrue positives:   14\tFalse positives:   71\tFalse negatives:  186\tTrue negatives: 1229\n",
      "\n",
      "Decision Tree, PCA 5 grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            m...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.83000\tPrecision: 0.20430\tRecall: 0.09500\tF1: 0.12969\tF2: 0.10638\n",
      "\tTotal predictions: 1500\tTrue positives:   19\tFalse positives:   74\tFalse negatives:  181\tTrue negatives: 1226\n",
      "\n",
      "Decision Tree, PCA 7 grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            m...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.81867\tPrecision: 0.18966\tRecall: 0.11000\tF1: 0.13924\tF2: 0.12009\n",
      "\tTotal predictions: 1500\tTrue positives:   22\tFalse positives:   94\tFalse negatives:  178\tTrue negatives: 1206\n",
      "\n",
      "Decision Tree, PCA 9 grid search\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            m...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.81267\tPrecision: 0.18110\tRecall: 0.11500\tF1: 0.14067\tF2: 0.12406\n",
      "\tTotal predictions: 1500\tTrue positives:   23\tFalse positives:  104\tFalse negatives:  177\tTrue negatives: 1196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_components in [None, 5, 7, 9]:\n",
    "    if n_components == None:\n",
    "        print \"Decision Tree grid search, no PCA\"\n",
    "        dtc = DecisionTreeClassifier()\n",
    "        param_grid = {'min_samples_split': [20,50,70]}\n",
    "        clf = GridSearchCV(dtc, param_grid)\n",
    "        test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "    else:\n",
    "        print \"Decision Tree, PCA {} grid search\".format(n_components)\n",
    "        pca = PCA(n_components)\n",
    "        param_grid = {'min_samples_split': [20,50,70]}\n",
    "        grid_dtc = GridSearchCV(dtc, param_grid)\n",
    "        clf = make_pipeline(pca, grid_dtc)\n",
    "        test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost with Naive Bayes and PCA\n",
    "\n",
    "I will try Adaboost with Naive Bayes as the base estimator, and use PCA with 9 components.  The highest F1 score I see is 0.23110, using 10 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost 2, Naive Bayes, PCA 9\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=GaussianNB(priors=None), learning_rate=1.0,\n",
      "          n_estimators=2, random_state=None))])\n",
      "\tAccuracy: 0.19400\tPrecision: 0.12210\tRecall: 0.81500\tF1: 0.21238\tF2: 0.38173\n",
      "\tTotal predictions: 1500\tTrue positives:  163\tFalse positives: 1172\tFalse negatives:   37\tTrue negatives:  128\n",
      "\n",
      "\n",
      "\n",
      "Adaboost 5, Naive Bayes, PCA 9\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=GaussianNB(priors=None), learning_rate=1.0,\n",
      "          n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.68267\tPrecision: 0.16827\tRecall: 0.35000\tF1: 0.22727\tF2: 0.28783\n",
      "\tTotal predictions: 1500\tTrue positives:   70\tFalse positives:  346\tFalse negatives:  130\tTrue negatives:  954\n",
      "\n",
      "\n",
      "\n",
      "Adaboost 10, Naive Bayes, PCA 9\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=GaussianNB(priors=None), learning_rate=1.0,\n",
      "          n_estimators=10, random_state=None))])\n",
      "\tAccuracy: 0.64067\tPrecision: 0.16168\tRecall: 0.40500\tF1: 0.23110\tF2: 0.31130\n",
      "\tTotal predictions: 1500\tTrue positives:   81\tFalse positives:  420\tFalse negatives:  119\tTrue negatives:  880\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_estimators in [2,5,10]:\n",
    "    n_components = 9\n",
    "    pca = PCA(n_components)\n",
    "    gnb = GaussianNB()\n",
    "    ada = AdaBoostClassifier(base_estimator=gnb, \n",
    "                                     n_estimators=n_estimators, \n",
    "                                     algorithm = algorithm)\n",
    "    clf = make_pipeline(pca,ada)\n",
    "    print \"Adaboost {}, Naive Bayes, PCA {}\".format(n_estimators, n_components)\n",
    "    test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost with SVM and PCA\n",
    "\n",
    "Given that I got better results using SVM with PCA, I will try to use this within Adaboost, and fix SVM parameters C = 5000, gamma = .005\n",
    "\n",
    "Note that if adaboost algorithm is set to the default SAMME.R, then it requires the weak learner (base estimator) to support calculation of class probabilities (it needs the base estimator to have the attribute 'predict_proba'.  \n",
    "\n",
    "Since SVM does not have this, I need to set algorithm to SAMME.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "Also, I cannot use grid search as the base_estimator because it does not support sample weights.\n",
    "\n",
    "Using AdaBoost with SVM, the highest F1 score I see is 0.34270 when there are either 5 or 10 estimators, PCA 5 components, (C is 5000, gamma is .005 were set for all iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimators 2 PCA 5, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=2, random_state=None))])\n",
      "\tAccuracy: 0.83067\tPrecision: 0.34483\tRecall: 0.30000\tF1: 0.32086\tF2: 0.30801\n",
      "\tTotal predictions: 1500\tTrue positives:   60\tFalse positives:  114\tFalse negatives:  140\tTrue negatives: 1186\n",
      "\n",
      "estimators 2 PCA 7, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=2, random_state=None))])\n",
      "\tAccuracy: 0.82800\tPrecision: 0.32738\tRecall: 0.27500\tF1: 0.29891\tF2: 0.28409\n",
      "\tTotal predictions: 1500\tTrue positives:   55\tFalse positives:  113\tFalse negatives:  145\tTrue negatives: 1187\n",
      "\n",
      "estimators 2 PCA 9, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=2, random_state=None))])\n",
      "\tAccuracy: 0.82867\tPrecision: 0.34078\tRecall: 0.30500\tF1: 0.32190\tF2: 0.31154\n",
      "\tTotal predictions: 1500\tTrue positives:   61\tFalse positives:  118\tFalse negatives:  139\tTrue negatives: 1182\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.84400\tPrecision: 0.39103\tRecall: 0.30500\tF1: 0.34270\tF2: 0.31904\n",
      "\tTotal predictions: 1500\tTrue positives:   61\tFalse positives:   95\tFalse negatives:  139\tTrue negatives: 1205\n",
      "\n",
      "estimators 5 PCA 7, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.83267\tPrecision: 0.33962\tRecall: 0.27000\tF1: 0.30084\tF2: 0.28154\n",
      "\tTotal predictions: 1500\tTrue positives:   54\tFalse positives:  105\tFalse negatives:  146\tTrue negatives: 1195\n",
      "\n",
      "estimators 5 PCA 9, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.83467\tPrecision: 0.35714\tRecall: 0.30000\tF1: 0.32609\tF2: 0.30992\n",
      "\tTotal predictions: 1500\tTrue positives:   60\tFalse positives:  108\tFalse negatives:  140\tTrue negatives: 1192\n",
      "\n",
      "estimators 10 PCA 5, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...rue,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))])\n",
      "\tAccuracy: 0.84400\tPrecision: 0.39103\tRecall: 0.30500\tF1: 0.34270\tF2: 0.31904\n",
      "\tTotal predictions: 1500\tTrue positives:   61\tFalse positives:   95\tFalse negatives:  139\tTrue negatives: 1205\n",
      "\n",
      "estimators 10 PCA 7, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...rue,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))])\n",
      "\tAccuracy: 0.83267\tPrecision: 0.33962\tRecall: 0.27000\tF1: 0.30084\tF2: 0.28154\n",
      "\tTotal predictions: 1500\tTrue positives:   54\tFalse positives:  105\tFalse negatives:  146\tTrue negatives: 1195\n",
      "\n",
      "estimators 10 PCA 9, Adaboost using SVC C 5000 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=9, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_funct...rue,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None))])\n",
      "\tAccuracy: 0.83467\tPrecision: 0.35714\tRecall: 0.30000\tF1: 0.32609\tF2: 0.30992\n",
      "\tTotal predictions: 1500\tTrue positives:   60\tFalse positives:  108\tFalse negatives:  140\tTrue negatives: 1192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for n_estimators in [2, 5, 10]:\n",
    "    for n_components in [5, 7, 9]:\n",
    "        C = 5000\n",
    "        gamma = .005\n",
    "        algorithm = 'SAMME'\n",
    "        print \"estimators {} PCA {}, Adaboost using SVC C {} gamma {}\".format(n_estimators,\n",
    "                                                                                     n_components,\n",
    "                                                                                     C,\n",
    "                                                                                     gamma)\n",
    "        pca = PCA(n_components=n_components)\n",
    "        base_estimator = svc = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "        ada = AdaBoostClassifier(base_estimator=base_estimator, \n",
    "                                 n_estimators=n_estimators, \n",
    "                                 algorithm = algorithm)\n",
    "        clf = make_pipeline(pca,ada)\n",
    "        test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll try Adaboost again with 5 estimators 5 PCA components.  I'll use loops to vary C and gamma.  \n",
    "Using SVC C 10000.0 gamma 0.005, I get an F1 of .36, precision of .36, and recall of .36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimators 5 PCA 5, Adaboost using SVC C 5000.0 gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fun...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.86600\tPrecision: 0.49587\tRecall: 0.30000\tF1: 0.37383\tF2: 0.32573\n",
      "\tTotal predictions: 1500\tTrue positives:   60\tFalse positives:   61\tFalse negatives:  140\tTrue negatives: 1239\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 5000.0 gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fun...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.87667\tPrecision: 0.74194\tRecall: 0.11500\tF1: 0.19913\tF2: 0.13839\n",
      "\tTotal predictions: 1500\tTrue positives:   23\tFalse positives:    8\tFalse negatives:  177\tTrue negatives: 1292\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 5000.0 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=5000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fun...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.84400\tPrecision: 0.39103\tRecall: 0.30500\tF1: 0.34270\tF2: 0.31904\n",
      "\tTotal predictions: 1500\tTrue positives:   61\tFalse positives:   95\tFalse negatives:  139\tTrue negatives: 1205\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 10000.0 gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.87667\tPrecision: 0.74194\tRecall: 0.11500\tF1: 0.19913\tF2: 0.13839\n",
      "\tTotal predictions: 1500\tTrue positives:   23\tFalse positives:    8\tFalse negatives:  177\tTrue negatives: 1292\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 10000.0 gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.86267\tPrecision: 0.46739\tRecall: 0.21500\tF1: 0.29452\tF2: 0.24103\n",
      "\tTotal predictions: 1500\tTrue positives:   43\tFalse positives:   49\tFalse negatives:  157\tTrue negatives: 1251\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 10000.0 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.83000\tPrecision: 0.36181\tRecall: 0.36000\tF1: 0.36090\tF2: 0.36036\n",
      "\tTotal predictions: 1500\tTrue positives:   72\tFalse positives:  127\tFalse negatives:  128\tTrue negatives: 1173\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 50000.0 gamma 0.0005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=50000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.84400\tPrecision: 0.39103\tRecall: 0.30500\tF1: 0.34270\tF2: 0.31904\n",
      "\tTotal predictions: 1500\tTrue positives:   61\tFalse positives:   95\tFalse negatives:  139\tTrue negatives: 1205\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 50000.0 gamma 0.001\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=50000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.83267\tPrecision: 0.36788\tRecall: 0.35500\tF1: 0.36132\tF2: 0.35750\n",
      "\tTotal predictions: 1500\tTrue positives:   71\tFalse positives:  122\tFalse negatives:  129\tTrue negatives: 1178\n",
      "\n",
      "\n",
      "\n",
      "estimators 5 PCA 5, Adaboost using SVC C 50000.0 gamma 0.005\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=50000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.80067\tPrecision: 0.29960\tRecall: 0.37000\tF1: 0.33110\tF2: 0.35339\n",
      "\tTotal predictions: 1500\tTrue positives:   74\tFalse positives:  173\tFalse negatives:  126\tTrue negatives: 1127\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_estimators = 5\n",
    "n_components = 5\n",
    "algorithm = 'SAMME'\n",
    "for C in [5e3, 1e4, 5e4]:\n",
    "    for gamma in [0.0005, 0.001, 0.005]:\n",
    "\n",
    "        print \"estimators {} PCA {}, Adaboost using SVC C {} gamma {}\".format(n_estimators,\n",
    "                                                                                     n_components,\n",
    "                                                                                     C,\n",
    "                                                                                     gamma)\n",
    "        pca = PCA(n_components=n_components)\n",
    "        base_estimator = svc = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "        ada = AdaBoostClassifier(base_estimator=base_estimator, \n",
    "                                 n_estimators=n_estimators, \n",
    "                                 algorithm = algorithm)\n",
    "        clf = make_pipeline(pca,ada)\n",
    "        test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost SVM with GridSearch doesn't appear to work, because it says that the result is worse than random, and cannot be fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_estimators = 5\n",
    "n_components = 5\n",
    "algorithm = 'SAMME'\n",
    "param_grid = {\n",
    "         'base_estimator__C': [5e3, 1e4, 5e4, 1e5],\n",
    "          'base_estimator__gamma': [0.0005, 0.001, 0.005, 0.01, 0.1],\n",
    "          }\n",
    "pca = PCA(n_components=n_components)\n",
    "base_estimator = SVC(kernel='rbf', class_weight='balanced')\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator, \n",
    "                                 n_estimators=n_estimators, \n",
    "                                 algorithm = algorithm)\n",
    "grid = GridSearchCV(ada, param_grid)\n",
    "clf = grid\n",
    "clf = make_pipeline(pca, grid)\n",
    "\n",
    "#this gives the error :\n",
    "#ValueError: BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.\n",
    "\n",
    "#test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost with Decision Tree\n",
    "\n",
    "Since the decision tree did better without PCA, I'll try Adaboost using Decision Tree as the base estimator, and using original features without PCA.\n",
    "\n",
    "The highest F1 score I see here is 0.33429, when n_estimators for Adaboost is 5, and min samples for a split in the tree is 70.  This seems to change each time i run the test, but generally, with 10 estimators, for min sample split of 20, 50 or 70, the F1 is .30 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost w/ Decision Tree; n_estimators 5, min_samples 20\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None)\n",
      "\tAccuracy: 0.84467\tPrecision: 0.37594\tRecall: 0.25000\tF1: 0.30030\tF2: 0.26795\n",
      "\tTotal predictions: 1500\tTrue positives:   50\tFalse positives:   83\tFalse negatives:  150\tTrue negatives: 1217\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 5, min_samples 50\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None)\n",
      "\tAccuracy: 0.83333\tPrecision: 0.34756\tRecall: 0.28500\tF1: 0.31319\tF2: 0.29564\n",
      "\tTotal predictions: 1500\tTrue positives:   57\tFalse positives:  107\tFalse negatives:  143\tTrue negatives: 1193\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 5, min_samples 70\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None)\n",
      "\tAccuracy: 0.84600\tPrecision: 0.39456\tRecall: 0.29000\tF1: 0.33429\tF2: 0.30623\n",
      "\tTotal predictions: 1500\tTrue positives:   58\tFalse positives:   89\tFalse negatives:  142\tTrue negatives: 1211\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 10, min_samples 20\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None)\n",
      "\tAccuracy: 0.84533\tPrecision: 0.38889\tRecall: 0.28000\tF1: 0.32558\tF2: 0.29661\n",
      "\tTotal predictions: 1500\tTrue positives:   56\tFalse positives:   88\tFalse negatives:  144\tTrue negatives: 1212\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 10, min_samples 50\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None)\n",
      "\tAccuracy: 0.84000\tPrecision: 0.36301\tRecall: 0.26500\tF1: 0.30636\tF2: 0.28013\n",
      "\tTotal predictions: 1500\tTrue positives:   53\tFalse positives:   93\tFalse negatives:  147\tTrue negatives: 1207\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 10, min_samples 70\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=10, random_state=None)\n",
      "\tAccuracy: 0.84467\tPrecision: 0.38129\tRecall: 0.26500\tF1: 0.31268\tF2: 0.28222\n",
      "\tTotal predictions: 1500\tTrue positives:   53\tFalse positives:   86\tFalse negatives:  147\tTrue negatives: 1214\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 20, min_samples 20\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=20, random_state=None)\n",
      "\tAccuracy: 0.84600\tPrecision: 0.37795\tRecall: 0.24000\tF1: 0.29358\tF2: 0.25890\n",
      "\tTotal predictions: 1500\tTrue positives:   48\tFalse positives:   79\tFalse negatives:  152\tTrue negatives: 1221\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 20, min_samples 50\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=20, random_state=None)\n",
      "\tAccuracy: 0.84600\tPrecision: 0.38519\tRecall: 0.26000\tF1: 0.31045\tF2: 0.27807\n",
      "\tTotal predictions: 1500\tTrue positives:   52\tFalse positives:   83\tFalse negatives:  148\tTrue negatives: 1217\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost w/ Decision Tree; n_estimators 20, min_samples 70\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=20, random_state=None)\n",
      "\tAccuracy: 0.83467\tPrecision: 0.32857\tRecall: 0.23000\tF1: 0.27059\tF2: 0.24468\n",
      "\tTotal predictions: 1500\tTrue positives:   46\tFalse positives:   94\tFalse negatives:  154\tTrue negatives: 1206\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "algorithm = 'SAMME.R' #default\n",
    "for n_estimators in [5,10,20]:\n",
    "    for min_samples_split in [20,50,70]:\n",
    "        base_estimator = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "        clf = AdaBoostClassifier(base_estimator=base_estimator, \n",
    "                                 n_estimators=n_estimators, \n",
    "                                 algorithm = algorithm)\n",
    "        print \"AdaBoost w/ Decision Tree; n_estimators {}, min_samples {}\".format(n_estimators,\n",
    "                                                                                 min_samples_split)\n",
    "        test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "With Random Forest, the highest F1 I see is 0.32117, with 5 estimators and 20 min sample size for a split.  This is better than using the decision tree by itself, for which the highest F1 score I found was .24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest n_estimators 5, min_samples 20\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=5, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86067\tPrecision: 0.45161\tRecall: 0.21000\tF1: 0.28669\tF2: 0.23516\n",
      "\tTotal predictions: 1500\tTrue positives:   42\tFalse positives:   51\tFalse negatives:  158\tTrue negatives: 1249\n",
      "\n",
      "\n",
      "\n",
      "Random Forest n_estimators 5, min_samples 50\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=5, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86733\tPrecision: 0.50847\tRecall: 0.15000\tF1: 0.23166\tF2: 0.17462\n",
      "\tTotal predictions: 1500\tTrue positives:   30\tFalse positives:   29\tFalse negatives:  170\tTrue negatives: 1271\n",
      "\n",
      "\n",
      "\n",
      "Random Forest n_estimators 5, min_samples 70\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=5, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.87333\tPrecision: 0.62500\tRecall: 0.12500\tF1: 0.20833\tF2: 0.14881\n",
      "\tTotal predictions: 1500\tTrue positives:   25\tFalse positives:   15\tFalse negatives:  175\tTrue negatives: 1285\n",
      "\n",
      "\n",
      "\n",
      "Random Forest n_estimators 10, min_samples 20\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.87600\tPrecision: 0.59459\tRecall: 0.22000\tF1: 0.32117\tF2: 0.25172\n",
      "\tTotal predictions: 1500\tTrue positives:   44\tFalse positives:   30\tFalse negatives:  156\tTrue negatives: 1270\n",
      "\n",
      "\n",
      "\n",
      "Random Forest n_estimators 10, min_samples 50\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.87200\tPrecision: 0.60000\tRecall: 0.12000\tF1: 0.20000\tF2: 0.14286\n",
      "\tTotal predictions: 1500\tTrue positives:   24\tFalse positives:   16\tFalse negatives:  176\tTrue negatives: 1284\n",
      "\n",
      "\n",
      "\n",
      "Random Forest n_estimators 10, min_samples 70\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.87667\tPrecision: 0.80000\tRecall: 0.10000\tF1: 0.17778\tF2: 0.12121\n",
      "\tTotal predictions: 1500\tTrue positives:   20\tFalse positives:    5\tFalse negatives:  180\tTrue negatives: 1295\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for n_estimators in [5,10]:\n",
    "    for min_samples_split in [20, 50, 70]:\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                    min_samples_split=min_samples_split)\n",
    "        print \"Random Forest n_estimators {}, min_samples {}\".format(n_estimators,\n",
    "                                                                     min_samples_split)\n",
    "        test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost with logistic regression\n",
    "\n",
    "This posting pointed out that the Adaboost's base estimator should support class probabilities, and gave logistic regression as an example.\n",
    "\n",
    "#### Reference \n",
    "http://stackoverflow.com/questions/27107205/sklearn-ensemble-adaboostclassifier-cannot-accecpt-svm-as-base-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gridsearchcv', GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=LogisticRegression(C=1.0, ...     pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0))])\n",
      "\tAccuracy: 0.89000\tPrecision: 0.83019\tRecall: 0.22000\tF1: 0.34783\tF2: 0.25791\n",
      "\tTotal predictions: 1500\tTrue positives:   44\tFalse positives:    9\tFalse negatives:  156\tTrue negatives: 1291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "n_estimators = 5\n",
    "n_components = 5\n",
    "param_grid = {\n",
    "    'base_estimator__C' : [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "    'n_estimators' : [5, 10]\n",
    "}\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "base_estimator = LogisticRegression()\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator, \n",
    "                                 n_estimators=n_estimators, \n",
    "                                 algorithm = algorithm)\n",
    "grid = GridSearchCV(ada, param_grid)\n",
    "clf = make_pipeline(pca, grid)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of created features\n",
    "\n",
    "Compare Adaboost with SVM with the email ratio features, with just the email counts, and without the email data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost SVM with email ratio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.83000\tPrecision: 0.36181\tRecall: 0.36000\tF1: 0.36090\tF2: 0.36036\n",
      "\tTotal predictions: 1500\tTrue positives:   72\tFalse positives:  127\tFalse negatives:  128\tTrue negatives: 1173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "n_estimators = 5\n",
    "n_components = 5\n",
    "algorithm = 'SAMME'\n",
    "C = 1e4\n",
    "gamma = 0.005\n",
    "pca = PCA(n_components=n_components)\n",
    "base_estimator = svc = SVC(kernel='rbf',\n",
    "                           class_weight='balanced',\n",
    "                           C=C,\n",
    "                           gamma=gamma)\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator,\n",
    "                         n_estimators=n_estimators,\n",
    "                         algorithm = algorithm)\n",
    "clf = make_pipeline(pca,ada)\n",
    "test_classifier(clf=clf,\n",
    "                dataset=my_dataset,\n",
    "                feature_list=feature_list,\n",
    "                folds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost SVM with absolute email counts\n",
    "\n",
    "This doesn't get a result, possibly because it gives no positive predictions.  So absolute email counts are not helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "feature_to_scale = ['salary',\n",
    "                    'total_stock_value',\n",
    "                     'total_payments',\n",
    "                     'restricted_stock',\n",
    "                     'exercised_stock_options',\n",
    "                     'other',\n",
    "                     'bonus',\n",
    "                     'expenses',\n",
    "                     'from_this_person_to_poi',\n",
    "                     'from_poi_to_this_person',\n",
    "                     'shared_receipt_with_poi'\n",
    "                    ]\n",
    "\n",
    "for feature in feature_to_scale:\n",
    "    data_dict = scale_feature(data_dict, feature, feature + '_scaled')\n",
    "\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'shared_receipt_with_poi'\n",
    "                ]\n",
    "\n",
    "n_estimators = 5\n",
    "n_components = 5\n",
    "algorithm = 'SAMME'\n",
    "C = 1e4\n",
    "gamma = 0.005\n",
    "pca = PCA(n_components=n_components)\n",
    "base_estimator = svc = SVC(kernel='rbf',\n",
    "                           class_weight='balanced',\n",
    "                           C=C,\n",
    "                           gamma=gamma)\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator,\n",
    "                         n_estimators=n_estimators,\n",
    "                         algorithm = algorithm)\n",
    "clf = make_pipeline(pca,ada)\n",
    "test_classifier(clf=clf,\n",
    "                dataset=my_dataset,\n",
    "                feature_list=feature_list,\n",
    "                folds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost SVM with without email data\n",
    "\n",
    "With only compensation data, the precision is higher, at .45, but recall is lower at .275.  This makes sense, in that with only compensation features, we cast a narrower net, and predict fewer false positives (and probably fewer true positives as well).  Moreover, without the email features, we miss more cases and have a lower recall.  The F1 score was 0.34."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('adaboostclassifier', AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_fu...True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.85933\tPrecision: 0.45455\tRecall: 0.27500\tF1: 0.34268\tF2: 0.29859\n",
      "\tTotal predictions: 1500\tTrue positives:   55\tFalse positives:   66\tFalse negatives:  145\tTrue negatives: 1234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled'\n",
    "                ]\n",
    "\n",
    "n_estimators = 5\n",
    "n_components = 5\n",
    "algorithm = 'SAMME'\n",
    "C = 1e4\n",
    "gamma = 0.005\n",
    "pca = PCA(n_components=n_components)\n",
    "base_estimator = svc = SVC(kernel='rbf',\n",
    "                           class_weight='balanced',\n",
    "                           C=C,\n",
    "                           gamma=gamma)\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator,\n",
    "                         n_estimators=n_estimators,\n",
    "                         algorithm = algorithm)\n",
    "clf = make_pipeline(pca,ada)\n",
    "test_classifier(clf=clf,\n",
    "                dataset=my_dataset,\n",
    "                feature_list=feature_list,\n",
    "                folds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Persons of Interest using Machine Learning\n",
    "\n",
    "### Eddy Shyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of project goal\n",
    "\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "Given information about Enron employees and whether they are persons of interest (POI) in fraud, we use machine learning to predict whether employees are persons of interest or not.  The data is primarily numerical, and either compensation related data (salary, bonus, stock options) or email count data (messages sent to persons of interest, all messages sent, etc). There are 144 persons in the dataset (after removing two non-person records), of which 18 are person of interest and the rest are not.  Some features have 100 or more missing values (deferral_payments, restricted_stock_deferred, director_fees, loan_advances), so we'll focus on features that have fewer missing values.  Compensation and email data can represent latent features, such as how similar a person is to a POI, or how professionally connected one is to a POI.  We will be using 11 features, 8 related to compensation, and 3 features related to email counts.\n",
    "\n",
    "Each record should represent a person.  Using a histogram of salary, I found an outlier named \"TOTAL\", which I removed, since it is not a person.  I also removed \"The Travel Agency in the Park\" because it also does not represent a person.  Outliers that represent persons are kept (for example, Jeff Skilling's salary was an outlier that was kept).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "\n",
    "I chose features that represent compensation and email counts, and excluded features that had more than a threshold of missing values.  When a feature has too many missing values, it can unintentionally associate missing values to a particular class.  When choosing the threshold, I wanted to include features that had fewer missing values than the core features that I wanted to keep (email counts), which was 60 missing values out of 144 total records.  The \"bonus\" feature was close enough to the threshold that I kept it as well.  So I selected 11 features, 8 that are compensation-related, and 3 that are email-related.\n",
    "\n",
    "I normalized emails sent to persons of interest (POI) by dividing by total number of emails sent; similarly for emails received from POI and emails received that were also sent to a POI.  This represents the fraction of each person's emails that were associated with a POI.  The latent feature I think that this represents is how close the professional relationship was with POIs.  I scaled all of the compensation and email count ratios to range from 0 to 1, to accommodate algorithms that calculate distances using features, such as SVM.  \n",
    "\n",
    "I used a random forest regression and also selectKBest algorithm to rank features by importance (highest importance first).\n",
    "\n",
    "feature rank using random forest\n",
    "\n",
    "[(0.16, 'expenses_scaled'), (0.16, 'exercised_stock_options_scaled'), (0.14, 'to_poi_ratio_scaled'), (0.1, 'shared_poi_ratio_scaled'), (0.1, 'other_scaled'), (0.09, 'salary_scaled'), (0.09, 'bonus_scaled'), (0.07, 'total_payments_scaled'), (0.04, 'total_stock_value_scaled'), (0.04, 'from_poi_ratio_scaled'), (0.02, 'restricted_stock_scaled')]\n",
    "\n",
    "feature rank using SelectKBest\n",
    "\n",
    "[(16.18, 'to_poi_ratio_scaled'), (9.02, 'shared_poi_ratio_scaled'), (3.05, 'from_poi_ratio_scaled'), (0.33, 'total_payments_scaled'), (0.21, 'exercised_stock_options_scaled'), (0.15, 'total_stock_value_scaled'), (0.11, 'restricted_stock_scaled'), (0.06, 'other_scaled'), (0.06, 'bonus_scaled'), (0.02, 'expenses_scaled'), (0.0, 'salary_scaled')]\n",
    "\n",
    "For random forest feature ranking, I used the top 1 through 6 features (top 1, top 2, top 3 etc).  I used SVM to test. The best F1 score was from using the top three features: to poi ratio, bonus, and shared poi ratio.\n",
    "\n",
    "For selectKbest ranking, I used the top 4,5,6 features, and SVM to test.  The best F1 score was from using the top 4 features: to poi ratio; shared poi ratio, from poi ratio, total payments.\n",
    "\n",
    "Since I got the highest score using the top three features found by random forest ranking, I used either the top three features (to poi email ratio, bonus, shared email ratio), as well as all 11 features when comparing algorithms.  This is because decision trees and random forest performed better with all 11 features.\n",
    "\n",
    "\n",
    "I also used SVM to test whether the new features improve prediction. \n",
    "\n",
    "\n",
    "#### SVM with top 3 features including email ratio:\n",
    "\n",
    "Precision: 0.44888\tRecall: 0.73100\tF1: 0.55621\n",
    "\n",
    "Features used are: to poi ratio, bonus, shared poi ratio\n",
    "\n",
    "#### SVM without new features\n",
    "\n",
    "Precision: 0.18282\tRecall: 0.11600\tF1: 0.14194\n",
    "\n",
    "Features used are bonus, exercised stock options, from poi email count, salary\n",
    "\n",
    "I used original features, ranked them using random forest, and used the top four from the ranking:\n",
    "\n",
    "[(0.16, 'bonus_scaled'), (0.14, 'exercised_stock_options_scaled'), (0.11, 'from_poi_to_this_person'), (0.1, 'salary_scaled'), (0.08, 'shared_receipt_with_poi'), (0.08, 'other_scaled'), (0.08, 'from_messages'), (0.07, 'expenses_scaled'), (0.05, 'total_payments_scaled'), (0.05, 'from_this_person_to_poi'), (0.03, 'restricted_stock_scaled'), (0.02, 'to_messages'), (0.01, 'total_stock_value_scaled')]\n",
    "\n",
    "I tuned parameters for SVM and tested.  The model performs worse without the new email ratio features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm selection\n",
    "\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "I chose SVM, as it had the highest F1 score.  SVM had the highest recall, so it would be good for casting a wider net to find more potential POIs.  Adaboost with logistic regression had the highest precision, so it would be good for trying to label only the actual POIs as POIs.\n",
    "\n",
    "I tried the following algorithms, using grid search to tune parameters and try with or without PCA.  The results are:\n",
    "\n",
    "#### SVM\n",
    "Precision: 0.44614\tRecall: 0.82000\tF1: 0.57787\n",
    "\n",
    "SVM does better with the top 3 features rather than all 11\n",
    "\n",
    "#### Adaboost and SVM\n",
    "Precision: 0.43168\tRecall: 0.65400\tF1: 0.52008\n",
    "\n",
    "Like SVM, Adaboost using SVM does best with the top selected features.\n",
    "Adaboost with SVM does slightly worse than SVM alone.\n",
    "\n",
    "#### Adaboost Decision Tree\n",
    "\n",
    "Precision: 0.40729\tRecall: 0.31850\tF1: 0.35746\n",
    "\n",
    "Similar to decision trees, adaboost using decision trees performs better using 11 features rather than the top 2\n",
    "\n",
    "#### Naive Bayes\n",
    "\n",
    "Precision: 0.39823\tRecall: 0.24750\tF1: 0.30527\n",
    "\n",
    "Naive Bayes performs better with selected features rather than all of them\n",
    "\n",
    "#### Adaboost with logistic regression\n",
    "\n",
    "Precision: 0.48645\tRecall: 0.18850\tF1: 0.27171\n",
    "\n",
    "Adaboost with logistic regression does a little bit better when using just the top 3 features as opposed to 11.\n",
    "The three algorithms that had precision, recall and F1 above 0.30 are SVM, Adaboost with SVM, and Adaboost with decision trees.\n",
    "\n",
    "#### Adaboost with Naive Bayes\n",
    "\n",
    "Precision: 0.31329\tRecall: 0.22400\tF1: 0.26122\n",
    "\n",
    "Adaboost with Naive Bayes did a little better with all 11 features rather than the top 3 features.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "Precision: 0.38034\tRecall: 0.14700\tF1: 0.21204\n",
    "Random forest with 11 features does worse than Adaboost with decision trees and 11 features, but better than decision trees alone.\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "Precision: 0.21442\tRecall: 0.16650\tF1: 0.18745\n",
    "\n",
    "Decision tree appears to do worse with just the top features, and when using PCA. Decision trees did better when I included all features.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]\n",
    "\n",
    "An algorithm's parameters are constants that can be altered to change the bias and variance of the model.  For instance, the SVM's C \"penalty\" can be increased to penalize mis-classifying each data point.  A higher penalty would make the model fit the training data more accurately, but also increase the variance of the model (a different training set would likely change the model significantly).  The SVM's gamma can be increased to reduce the influence of a data point in determining the classification of other points that are far away; this increases the model's bias, so that a different training set is not likely to result in a different model.\n",
    "\n",
    "When using PCA, I varied the number of components used, and also tested on the original features without PCA.  For Adaboost, I varied the number of estimator iterations.  For SVM, I varied the C (penalty) and gamma.  For decision trees, I varied the minimum number of samples to allow a split.  For random forest, I varied the number of trees.  For logistic regression, I varied the penalty parameter.\n",
    "\n",
    "#### Reference:\n",
    "https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]\n",
    "\n",
    "Validation measures how well the model would perform when making new predictions.  Validation uses test data that is not part of the training data.  If the same data is used to train and validate a model, then the parameters that give the best validation score will also cause the model to overfit the data, resulting in a high variance model. A high variance model would change significantly each time it is given a different data set to train on.  Another requirement is for the training and test data to be chosen randomly, so that both sets are representative of the whole data set.  If training and test data are not representative of the whole set, then the model will perform poorly when validated against the test data. For example, if all training data are of POI and all test data are of non-POIs, then the model will make poor predictions when faced with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "The precision score measures what fraction of all predictions of POI are actually POI.\n",
    "\n",
    "The recall score measures what fraction of all actual POIs are correctly predicted as POI.\n",
    "\n",
    "An F1 score equally weighs the precision and recall as one score.  So a model has a higher F1 score when more of its predictions of POI are actually POI, and also when more of the actual POI are predicted as POI.\n",
    "\n",
    "The cross validation method used is stratified shuffle split.  Stratified validation is useful when the number of POIs and non-POIs are very different as a fraction of the available data (for example, we have few POIs and many non-POIs).  When dividing the data into several folds (some of which are assigned to training, the rest for testing), each fold is stratified, meaning that the fraction of POIs in each fold is the same as the fraction in the whole data set.  The data is also shuffled before it is divided into folds.  Shuffling data also helps to make the training and test set more representative of the whole data set.\n",
    "\n",
    "#### Reference\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: steps taken to select features and choose algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edude/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from time import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sys.path.append(\"../tools/\")\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_l = defaultdict(list)\n",
    "for name, content in data_dict.iteritems():    \n",
    "    feature_l['name'].append(name)\n",
    "    for feature, value in content.iteritems():\n",
    "        feature_l[feature].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to_messages',\n",
       " 'deferral_payments',\n",
       " 'expenses',\n",
       " 'poi',\n",
       " 'long_term_incentive',\n",
       " 'email_address',\n",
       " 'from_poi_to_this_person',\n",
       " 'deferred_income',\n",
       " 'restricted_stock_deferred',\n",
       " 'shared_receipt_with_poi',\n",
       " 'loan_advances',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'director_fees',\n",
       " 'bonus',\n",
       " 'total_stock_value',\n",
       " 'from_this_person_to_poi',\n",
       " 'restricted_stock',\n",
       " 'salary',\n",
       " 'name',\n",
       " 'total_payments',\n",
       " 'exercised_stock_options']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_l.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['METTS MARK',\n",
       " 'BAXTER JOHN C',\n",
       " 'ELLIOTT STEVEN',\n",
       " 'CORDES WILLIAM R',\n",
       " 'HANNON KEVIN P',\n",
       " 'MORDAUNT KRISTINA M',\n",
       " 'MEYER ROCKFORD G',\n",
       " 'MCMAHON JEFFREY',\n",
       " 'HORTON STANLEY C',\n",
       " 'PIPER GREGORY F',\n",
       " 'HUMPHREY GENE E',\n",
       " 'UMANOFF ADAM S',\n",
       " 'BLACHMAN JEREMY M',\n",
       " 'SUNDE MARTIN',\n",
       " 'GIBBS DANA R',\n",
       " 'LOWRY CHARLES P',\n",
       " 'COLWELL WESLEY',\n",
       " 'MULLER MARK S',\n",
       " 'JACKSON CHARLENE R',\n",
       " 'WESTFAHL RICHARD K',\n",
       " 'WALTERS GARETH W',\n",
       " 'WALLS JR ROBERT H',\n",
       " 'KITCHEN LOUISE',\n",
       " 'CHAN RONNIE',\n",
       " 'BELFER ROBERT',\n",
       " 'SHANKMAN JEFFREY A',\n",
       " 'WODRASKA JOHN',\n",
       " 'BERGSIEKER RICHARD P',\n",
       " 'URQUHART JOHN A',\n",
       " 'BIBI PHILIPPE A',\n",
       " 'RIEKER PAULA H',\n",
       " 'WHALEY DAVID A',\n",
       " 'BECK SALLY W',\n",
       " 'HAUG DAVID L',\n",
       " 'ECHOLS JOHN B',\n",
       " 'MENDELSOHN JOHN',\n",
       " 'HICKERSON GARY J',\n",
       " 'CLINE KENNETH W',\n",
       " 'LEWIS RICHARD',\n",
       " 'HAYES ROBERT E',\n",
       " 'MCCARTY DANNY J',\n",
       " 'KOPPER MICHAEL J',\n",
       " 'LEFF DANIEL P',\n",
       " 'LAVORATO JOHN J',\n",
       " 'BERBERIAN DAVID',\n",
       " 'DETMERING TIMOTHY J',\n",
       " 'WAKEHAM JOHN',\n",
       " 'POWERS WILLIAM',\n",
       " 'GOLD JOSEPH',\n",
       " 'BANNANTINE JAMES M',\n",
       " 'DUNCAN JOHN H',\n",
       " 'SHAPIRO RICHARD S',\n",
       " 'SHERRIFF JOHN R',\n",
       " 'SHELBY REX',\n",
       " 'LEMAISTRE CHARLES',\n",
       " 'DEFFNER JOSEPH M',\n",
       " 'KISHKILL JOSEPH G',\n",
       " 'WHALLEY LAWRENCE G',\n",
       " 'MCCONNELL MICHAEL S',\n",
       " 'PIRO JIM',\n",
       " 'DELAINEY DAVID W',\n",
       " 'SULLIVAN-SHAKLOVITZ COLLEEN',\n",
       " 'WROBEL BRUCE',\n",
       " 'LINDHOLM TOD A',\n",
       " 'MEYER JEROME J',\n",
       " 'LAY KENNETH L',\n",
       " 'BUTTS ROBERT H',\n",
       " 'OLSON CINDY K',\n",
       " 'MCDONALD REBECCA',\n",
       " 'CUMBERLAND MICHAEL S',\n",
       " 'GAHN ROBERT S',\n",
       " 'MCCLELLAN GEORGE',\n",
       " 'HERMANN ROBERT J',\n",
       " 'SCRIMSHAW MATTHEW',\n",
       " 'GATHMANN WILLIAM D',\n",
       " 'HAEDICKE MARK E',\n",
       " 'BOWEN JR RAYMOND M',\n",
       " 'GILLIS JOHN',\n",
       " 'FITZGERALD JAY L',\n",
       " 'MORAN MICHAEL P',\n",
       " 'REDMOND BRIAN L',\n",
       " 'BAZELIDES PHILIP J',\n",
       " 'BELDEN TIMOTHY N',\n",
       " 'DURAN WILLIAM D',\n",
       " 'THORN TERENCE H',\n",
       " 'FASTOW ANDREW S',\n",
       " 'FOY JOE',\n",
       " 'CALGER CHRISTOPHER F',\n",
       " 'RICE KENNETH D',\n",
       " 'KAMINSKI WINCENTY J',\n",
       " 'LOCKHART EUGENE E',\n",
       " 'COX DAVID',\n",
       " 'OVERDYKE JR JERE C',\n",
       " 'PEREIRA PAULO V. FERRAZ',\n",
       " 'STABLER FRANK',\n",
       " 'SKILLING JEFFREY K',\n",
       " 'BLAKE JR. NORMAN P',\n",
       " 'SHERRICK JEFFREY B',\n",
       " 'PRENTICE JAMES',\n",
       " 'GRAY RODNEY',\n",
       " 'PICKERING MARK R',\n",
       " 'THE TRAVEL AGENCY IN THE PARK',\n",
       " 'NOLES JAMES L',\n",
       " 'KEAN STEVEN J',\n",
       " 'TOTAL',\n",
       " 'FOWLER PEGGY',\n",
       " 'WASAFF GEORGE',\n",
       " 'WHITE JR THOMAS E',\n",
       " 'CHRISTODOULOU DIOMEDES',\n",
       " 'ALLEN PHILLIP K',\n",
       " 'SHARP VICTORIA T',\n",
       " 'JAEDICKE ROBERT',\n",
       " 'WINOKUR JR. HERBERT S',\n",
       " 'BROWN MICHAEL',\n",
       " 'BADUM JAMES P',\n",
       " 'HUGHES JAMES A',\n",
       " 'REYNOLDS LAWRENCE',\n",
       " 'DIMICHELE RICHARD G',\n",
       " 'BHATNAGAR SANJAY',\n",
       " 'CARTER REBECCA C',\n",
       " 'BUCHANAN HAROLD G',\n",
       " 'YEAP SOON',\n",
       " 'MURRAY JULIA H',\n",
       " 'GARLAND C KEVIN',\n",
       " 'DODSON KEITH',\n",
       " 'YEAGER F SCOTT',\n",
       " 'HIRKO JOSEPH',\n",
       " 'DIETRICH JANET R',\n",
       " 'DERRICK JR. JAMES V',\n",
       " 'FREVERT MARK A',\n",
       " 'PAI LOU L',\n",
       " 'BAY FRANKLIN R',\n",
       " 'HAYSLETT RODERICK J',\n",
       " 'FUGH JOHN L',\n",
       " 'FALLON JAMES B',\n",
       " 'KOENIG MARK E',\n",
       " 'SAVAGE FRANK',\n",
       " 'IZZO LAWRENCE L',\n",
       " 'TILNEY ELIZABETH A',\n",
       " 'MARTIN AMANDA K',\n",
       " 'BUY RICHARD B',\n",
       " 'GRAMM WENDY L',\n",
       " 'CAUSEY RICHARD A',\n",
       " 'TAYLOR MITCHELL S',\n",
       " 'DONAHUE JR JEFFREY M',\n",
       " 'GLISAN JR BEN F']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_l['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the names, 'The Travel Agency in the Park' is not a person, so we'll remove this.  We are trying to identify persons of interest, so we only want to train and test on person data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert dict to a data frame to describe data and plot it\n",
    "data_df = pd.DataFrame(feature_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove NaN from each col and plot it for outliers\n",
    "salary = data_df['salary']\n",
    "salary_c = salary[salary.apply(lambda x: not math.isnan(float(x)))]\n",
    "salary_c = pd.DataFrame(salary_c.apply(lambda x: float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.621943e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.716369e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.770000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.118160e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.599960e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.121170e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.670423e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             salary\n",
       "count  9.500000e+01\n",
       "mean   5.621943e+05\n",
       "std    2.716369e+06\n",
       "min    4.770000e+02\n",
       "25%    2.118160e+05\n",
       "50%    2.599960e+05\n",
       "75%    3.121170e+05\n",
       "max    2.670423e+07"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_c.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram shows an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwXGRuC5kzsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f8b1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "salary_c.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26704229.0"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_c.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104    TOTAL\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['salary']==salary_c.max()[0]]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the 'TOTAL' record from data_dict and the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 97343619,\n",
       " 'deferral_payments': 32083396,\n",
       " 'deferred_income': -27992891,\n",
       " 'director_fees': 1398517,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'expenses': 5235198,\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 83925000,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'other': 42667589,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 130322299,\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'salary': 26704229,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 309886585,\n",
       " 'total_stock_value': 434509511}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop('TOTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df = data_df[data_df['name'] != 'TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = data_df[data_df['name'] != 'THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi = data_df['poi']\n",
    "poi = poi.apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove max from from salary_c and check again\n",
    "salary_c = salary_c[ salary_c['salary'] != salary_c['salary'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEKCAYAAADw2zkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH+9waK7NIicwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fa910d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "salary_c.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111258.0"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_c['salary'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95    SKILLING JEFFREY K\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[ data_df['salary'] == salary_c['salary'].max() ]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this second 'outlier' is a person, we'll keep this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for other outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expenses = data_df['expenses']\n",
    "expenses_c = expenses[expenses.apply(lambda x: not math.isnan(float(x)))]\n",
    "expenses_c = pd.DataFrame(expenses_c.apply(lambda x: float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQhkAklQoA0CSCvX/HAuAy9+EoVwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f0c4f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expenses_c.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54192.010638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46108.377454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22479.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46547.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78408.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>228763.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            expenses\n",
       "count      94.000000\n",
       "mean    54192.010638\n",
       "std     46108.377454\n",
       "min       148.000000\n",
       "25%     22479.000000\n",
       "50%     46547.500000\n",
       "75%     78408.500000\n",
       "max    228763.000000"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expenses_c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71    MCCLELLAN GEORGE\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[ data_df['expenses'] == expenses_c['expenses'].max() ]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max expense is from a person; we'll keep this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nan_count = defaultdict(list)\n",
    "for k, v in feature_l.iteritems():\n",
    "    num_nan = sum([1 for e in v if e == 'NaN'] )\n",
    "    nan_count['feature'].append(k)\n",
    "    nan_count['nan_count'].append(num_nan)\n",
    "    \n",
    "nan_count = pd.DataFrame(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_stock_value</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>total_payments</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>email_address</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>restricted_stock</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>exercised_stock_options</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>expenses</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>salary</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>other</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>from_this_person_to_poi</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to_messages</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shared_receipt_with_poi</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>from_poi_to_this_person</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from_messages</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bonus</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>long_term_incentive</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deferred_income</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deferral_payments</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>restricted_stock_deferred</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>director_fees</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>loan_advances</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  nan_count\n",
       "19                       name          0\n",
       "3                         poi          0\n",
       "15          total_stock_value         20\n",
       "20             total_payments         21\n",
       "5               email_address         35\n",
       "17           restricted_stock         36\n",
       "21    exercised_stock_options         44\n",
       "2                    expenses         51\n",
       "18                     salary         51\n",
       "12                      other         53\n",
       "16    from_this_person_to_poi         60\n",
       "0                 to_messages         60\n",
       "9     shared_receipt_with_poi         60\n",
       "6     from_poi_to_this_person         60\n",
       "11              from_messages         60\n",
       "14                      bonus         64\n",
       "4         long_term_incentive         80\n",
       "7             deferred_income         97\n",
       "1           deferral_payments        107\n",
       "8   restricted_stock_deferred        128\n",
       "13              director_fees        129\n",
       "10              loan_advances        142"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_count.sort_values('nan_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer features that have more data as opposed to missing data.  Since I plan to use the email data, which have 60 NaNs per feature at most, I'll use that as the cut-off point.  Since 'bonus' is close to that cut-off at 64 NaNs, I'll also include 'bonus' and all features with fewer NaNs.  I'll exclude 'email address', since it is not numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### feature_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "#The features are either related to compensation/expenses or to emails\n",
    "feature_list = ['poi',\n",
    "                 'salary',\n",
    "                 'total_stock_value',\n",
    "                 'total_payments',\n",
    "                 'restricted_stock',\n",
    "                 'exercised_stock_options',\n",
    "                 'other',\n",
    "                 'bonus',\n",
    "                 'expenses',\n",
    "                 'to_messages',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'shared_receipt_with_poi'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two members appear not to be persons, so I'll remove these from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new features\n",
    "\n",
    "I can scale the compensation data to be between 0 and 1.  This is helpful if using SVM or K-means which calculate a distance based on more than one dimension.\n",
    "\n",
    "For emails, I can get a ratio of poi emails received divided by all emails received, and similarly for other poi_emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scale_feature(data_dict, feature, feature_scaled):\n",
    "    feature_l = [v[feature] for v in data_dict.values() if v[feature] != 'NaN']\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.array(feature_l).reshape(len(feature_l),1))\n",
    "    \n",
    "    for name, data in data_dict.iteritems():\n",
    "        if data[feature] == 'NaN':\n",
    "            data[feature_scaled] = 'NaN'\n",
    "        else:\n",
    "            data[feature_scaled] = scaler.transform(np.array([[data[feature]]]))[0][0]\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ratio(data_dict, numerator, denominator, ratio):\n",
    "    for k, v in data_dict.iteritems():\n",
    "        n = v[numerator]\n",
    "        d = v[denominator]\n",
    "        if n == 'NaN' or d == 'NaN' or d == 0:\n",
    "            data_dict[k][ratio] = 'NaN'\n",
    "        else:\n",
    "            data_dict[k][ratio] = float(n) / float(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first compute email ratios\n",
    "ratios_to_compute = [('from_this_person_to_poi', 'from_messages', 'to_poi_ratio'),\n",
    "                     ('from_poi_to_this_person', 'to_messages', 'from_poi_ratio'),\n",
    "                     ('shared_receipt_with_poi', 'to_messages', 'shared_poi_ratio')\n",
    "                    ]\n",
    "\n",
    "for numerator, denominator, ratio in ratios_to_compute:\n",
    "    compute_ratio(data_dict, numerator, denominator, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#then scale compensation, expense, and email ratios\n",
    "feature_to_scale = ['salary',\n",
    "                    'total_stock_value',\n",
    "                     'total_payments',\n",
    "                     'restricted_stock',\n",
    "                     'exercised_stock_options',\n",
    "                     'other',\n",
    "                     'bonus',\n",
    "                     'expenses',\n",
    "                     'to_poi_ratio',\n",
    "                     'from_poi_ratio',\n",
    "                     'shared_poi_ratio'\n",
    "                    ]\n",
    "for feature in feature_to_scale:\n",
    "    data_dict = scale_feature(data_dict, feature, feature + '_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 600000,\n",
       " 'bonus_scaled': 0.0054485481824213819,\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'mark.metts@enron.com',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'exercised_stock_options_scaled': 'NaN',\n",
       " 'expenses': 94299,\n",
       " 'expenses_scaled': 0.017984737490568382,\n",
       " 'from_messages': 29,\n",
       " 'from_poi_ratio': 0.04708798017348203,\n",
       " 'from_poi_ratio_scaled': 0.21665480239394658,\n",
       " 'from_poi_to_this_person': 38,\n",
       " 'from_this_person_to_poi': 1,\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 1740,\n",
       " 'other_scaled': 4.0733496365754174e-05,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 585062,\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'restricted_stock_scaled': 0.023994802131269415,\n",
       " 'salary': 365788,\n",
       " 'salary_scaled': 0.013680137532733229,\n",
       " 'shared_poi_ratio': 0.8698884758364313,\n",
       " 'shared_poi_ratio_scaled': 0.86644162302202699,\n",
       " 'shared_receipt_with_poi': 702,\n",
       " 'to_messages': 807,\n",
       " 'to_poi_ratio': 0.034482758620689655,\n",
       " 'to_poi_ratio_scaled': 0.034482758620689655,\n",
       " 'total_payments': 1061827,\n",
       " 'total_payments_scaled': 0.0034260260315942775,\n",
       " 'total_stock_value': 585062,\n",
       " 'total_stock_value_scaled': 0.0014478190819469075}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[data_dict.keys()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank features by random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_only = feature_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature rank using random forest\n",
      "[(0.18, 'to_poi_ratio_scaled'), (0.14, 'bonus_scaled'), (0.12, 'shared_poi_ratio_scaled'), (0.11, 'total_stock_value_scaled'), (0.1, 'exercised_stock_options_scaled'), (0.09, 'total_payments_scaled'), (0.09, 'other_scaled'), (0.09, 'expenses_scaled'), (0.05, 'restricted_stock_scaled'), (0.02, 'from_poi_ratio_scaled'), (0.01, 'salary_scaled')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(features, labels)\n",
    "feature_rank = sorted (zip (map(lambda x: round(x,2),clf.feature_importances_) , feature_only), reverse=True)\n",
    "print \"feature rank using random forest\"\n",
    "print feature_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.61657\tPrecision: 0.26894\tRecall: 0.98000\tF1: 0.42205\tF2: 0.64103\n",
      "\tTotal predictions: 7000\tTrue positives:  980\tFalse positives: 2664\tFalse negatives:   20\tTrue negatives: 3336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 1 feature of random forest ranking\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.72020\tPrecision: 0.40527\tRecall: 0.85350\tF1: 0.54958\tF2: 0.69890\n",
      "\tTotal predictions: 10000\tTrue positives: 1707\tFalse positives: 2505\tFalse negatives:  293\tTrue negatives: 5495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 2 features of random forest ranking\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.78218\tPrecision: 0.44614\tRecall: 0.82000\tF1: 0.57787\tF2: 0.70230\n",
      "\tTotal predictions: 11000\tTrue positives: 1640\tFalse positives: 2036\tFalse negatives:  360\tTrue negatives: 6964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 3 features of random forest ranking\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.76469\tPrecision: 0.35959\tRecall: 0.67800\tF1: 0.46994\tF2: 0.57599\n",
      "\tTotal predictions: 13000\tTrue positives: 1356\tFalse positives: 2415\tFalse negatives:  644\tTrue negatives: 8585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 4 features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'total_stock_value_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.76131\tPrecision: 0.35634\tRecall: 0.68400\tF1: 0.46857\tF2: 0.57775\n",
      "\tTotal predictions: 13000\tTrue positives: 1368\tFalse positives: 2471\tFalse negatives:  632\tTrue negatives: 8529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 5 features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'total_stock_value_scaled',\n",
    "                'exercised_stock_options_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.75893\tPrecision: 0.31583\tRecall: 0.58950\tF1: 0.41130\tF2: 0.50243\n",
      "\tTotal predictions: 14000\tTrue positives: 1179\tFalse positives: 2554\tFalse negatives:  821\tTrue negatives: 9446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 6 features\n",
    "feature_list = ['poi',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'expenses_scaled',\n",
    "                'exercised_stock_options_scaled',\n",
    "                'total_stock_value_scaled',\n",
    "                'total_payments_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank feature using selectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature rank using SelectKBest\n",
      "[(16.18, 'to_poi_ratio_scaled'), (9.02, 'shared_poi_ratio_scaled'), (3.05, 'from_poi_ratio_scaled'), (0.33, 'total_payments_scaled'), (0.21, 'exercised_stock_options_scaled'), (0.15, 'total_stock_value_scaled'), (0.11, 'restricted_stock_scaled'), (0.06, 'other_scaled'), (0.06, 'bonus_scaled'), (0.02, 'expenses_scaled'), (0.0, 'salary_scaled')]\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "clf = SelectKBest(k=11)\n",
    "clf.fit(features, labels)\n",
    "feature_rank = sorted (zip (map(lambda x: round(x,2),clf.scores_), feature_only), reverse = True)\n",
    "print \"feature rank using SelectKBest\"\n",
    "print feature_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with top 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.67544\tPrecision: 0.24744\tRecall: 0.94100\tF1: 0.39184\tF2: 0.60297\n",
      "\tTotal predictions: 9000\tTrue positives:  941\tFalse positives: 2862\tFalse negatives:   59\tTrue negatives: 5138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 3 features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'from_poi_ratio_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.75836\tPrecision: 0.33794\tRecall: 0.72100\tF1: 0.46019\tF2: 0.58776\n",
      "\tTotal predictions: 14000\tTrue positives: 1442\tFalse positives: 2825\tFalse negatives:  558\tTrue negatives: 9175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 4 features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'from_poi_ratio_scaled',\n",
    "                'total_payments_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.75386\tPrecision: 0.32245\tRecall: 0.65650\tF1: 0.43248\tF2: 0.54382\n",
      "\tTotal predictions: 14000\tTrue positives: 1313\tFalse positives: 2759\tFalse negatives:  687\tTrue negatives: 9241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 5 features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'from_poi_ratio_scaled',\n",
    "                'total_payments_scaled',\n",
    "                'exercised_stock_options_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.73950\tPrecision: 0.29920\tRecall: 0.61350\tF1: 0.40223\tF2: 0.50698\n",
      "\tTotal predictions: 14000\tTrue positives: 1227\tFalse positives: 2874\tFalse negatives:  773\tTrue negatives: 9126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 6 features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'from_poi_ratio_scaled',\n",
    "                'total_payments_scaled',\n",
    "                'exercised_stock_options_scaled',\n",
    "                'total_stock_value_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results, I'll use the top 3 feaures as ranked by random forest:\n",
    "\n",
    "to poi ratio\n",
    "\n",
    "bonus\n",
    "\n",
    "shared poi ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Precision: 0.39823\tRecall: 0.24750\tF1: 0.30527\n",
    "\n",
    "Naive Bayes performs better with selected features rather than all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.79518\tPrecision: 0.39823\tRecall: 0.24750\tF1: 0.30527\tF2: 0.26777\n",
      "\tTotal predictions: 11000\tTrue positives:  495\tFalse positives:  748\tFalse negatives: 1505\tTrue negatives: 8252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "clf = GaussianNB()\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83867\tPrecision: 0.33465\tRecall: 0.21250\tF1: 0.25994\tF2: 0.22923\n",
      "\tTotal predictions: 15000\tTrue positives:  425\tFalse positives:  845\tFalse negatives: 1575\tTrue negatives: 12155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "clf = GaussianNB()\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "\n",
    "Precision: 0.44614\tRecall: 0.82000\tF1: 0.57787\n",
    "\n",
    "SVM does better with the top 3 features rather than all 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [500.0, 1000.0, 5000.0, 10000.0, 10000.0], 'gamma': [0.05, 0.1, 0.5, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid = {'C': [5e2, 1e3, 5e3, 1e4, 1e4],\n",
    "              'gamma': [.05, 0.1, 0.5, 1]\n",
    "             }\n",
    "grid = GridSearchCV(estimator=svc, param_grid=param_grid, cv=cv, scoring='f1')\n",
    "grid.fit(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 1000.0, 'gamma': 0.1} with a score of 0.570071428571\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.78218\tPrecision: 0.44614\tRecall: 0.82000\tF1: 0.57787\tF2: 0.70230\n",
      "\tTotal predictions: 11000\tTrue positives: 1640\tFalse positives: 2036\tFalse negatives:  360\tTrue negatives: 6964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top 3 features of random forest ranking\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "C = 1000\n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.80960\tPrecision: 0.26940\tRecall: 0.25000\tF1: 0.25934\tF2: 0.25365\n",
      "\tTotal predictions: 15000\tTrue positives:  500\tFalse positives: 1356\tFalse negatives: 1500\tTrue negatives: 11644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with 11 features\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "C = 10000 \n",
    "gamma = 0.5 \n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "Precision: 0.21442\tRecall: 0.16650\tF1: 0.18745\n",
    "\n",
    "Decision tree appears to do worse with just the top features, and when using PCA. Decision trees did better when I included all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [20, 40, 60, 80]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid = {'min_samples_split' : [20, 40, 60, 80]\n",
    "             }\n",
    "grid = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'min_samples_split': 20} with a score of 0.247428571429\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.77945\tPrecision: 0.20251\tRecall: 0.07250\tF1: 0.10677\tF2: 0.08318\n",
      "\tTotal predictions: 11000\tTrue positives:  145\tFalse positives:  571\tFalse negatives: 1855\tTrue negatives: 8429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                'shared_poi_ratio_scaled',\n",
    "                'bonus_scaled'\n",
    "                ]\n",
    "clf = DecisionTreeClassifier(min_samples_split=20)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  1.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [20, 40, 60, 80]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tune parameters with 11 features\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid = {'min_samples_split' : [20, 40, 60, 80]\n",
    "             }\n",
    "grid = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'min_samples_split': 20} with a score of 0.135904761905\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.80753\tPrecision: 0.21442\tRecall: 0.16650\tF1: 0.18745\tF2: 0.17429\n",
      "\tTotal predictions: 15000\tTrue positives:  333\tFalse positives: 1220\tFalse negatives: 1667\tTrue negatives: 11780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#try again with 11 features\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "clf = DecisionTreeClassifier(min_samples_split=20)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost with Naive Bayes\n",
    "\n",
    "Precision: 0.31329\tRecall: 0.22400\tF1: 0.26122\n",
    "\n",
    "Adaboost with Naive Bayes did a little better with all 11 features rather than the top 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME', base_estimator=GaussianNB(priors=None),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "ada = AdaBoostClassifier(base_estimator=gnb, algorithm='SAMME')\n",
    "n_iter=100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid={'n_estimators': [5,10]\n",
    "           }\n",
    "grid = GridSearchCV(estimator=ada, param_grid=param_grid, cv=cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_estimators': 5} with a score of 0.212547619048\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME', base_estimator=GaussianNB(priors=None),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None)\n",
      "\tAccuracy: 0.80045\tPrecision: 0.39617\tRecall: 0.18600\tF1: 0.25315\tF2: 0.20808\n",
      "\tTotal predictions: 11000\tTrue positives:  372\tFalse positives:  567\tFalse negatives: 1628\tTrue negatives: 8433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with select features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "clf = AdaBoostClassifier(base_estimator=gnb, algorithm='SAMME', n_estimators=5)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME', base_estimator=GaussianNB(priors=None),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None)\n",
      "\tAccuracy: 0.83107\tPrecision: 0.31329\tRecall: 0.22400\tF1: 0.26122\tF2: 0.23754\n",
      "\tTotal predictions: 15000\tTrue positives:  448\tFalse positives:  982\tFalse negatives: 1552\tTrue negatives: 12018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with 11 features\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "gnb = GaussianNB()\n",
    "clf = AdaBoostClassifier(base_estimator=gnb, algorithm='SAMME', n_estimators=5)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost and SVM\n",
    "\n",
    "Precision: 0.43168\tRecall: 0.65400\tF1: 0.52008\n",
    "\n",
    "Like SVM, Adaboost using SVM does best with the top selected features.\n",
    "Adaboost with SVM does slightly worse than SVM alone.\n",
    "\n",
    "Note that if adaboost algorithm is set to the default SAMME.R, then it requires the weak learner (base estimator) to support calculation of class probabilities (it needs the base estimator to have the attribute 'predict_proba'.  \n",
    "\n",
    "Since SVM does not have this, I need to set algorithm to SAMME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'base_estimator__C': [1000.0, 5000.0, 10000.0, 50000.0], 'n_estimators': [5, 10], 'base_estimator__gamma': [0.25, 0.5, 0.75]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "ada = AdaBoostClassifier(base_estimator=svc, algorithm = 'SAMME')\n",
    "\n",
    "param_grid = {\n",
    "    'base_estimator__C': [1e3, 5e3, 1e4, 5e4],\n",
    "    'base_estimator__gamma': [0.25, 0.5, 0.75],\n",
    "    'n_estimators' : [5, 10]\n",
    "          }\n",
    "grid = GridSearchCV(estimator=ada, param_grid=param_grid, cv = cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'base_estimator__C': 10000.0, 'n_estimators': 5, 'base_estimator__gamma': 0.25} with a score of 0.516777777778\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=SVC(C=10000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.25, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None)\n",
      "\tAccuracy: 0.78055\tPrecision: 0.43168\tRecall: 0.65400\tF1: 0.52008\tF2: 0.59293\n",
      "\tTotal predictions: 11000\tTrue positives: 1308\tFalse positives: 1722\tFalse negatives:  692\tTrue negatives: 7278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "C = 10000\n",
    "gamma = 0.25\n",
    "n_estimators = 5\n",
    "svc = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "clf = AdaBoostClassifier(base_estimator=svc, algorithm = 'SAMME', n_estimators=n_estimators)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Decision Tree\n",
    "\n",
    "Precision: 0.40729\tRecall: 0.31850\tF1: 0.35746\n",
    "\n",
    "Similar to decision trees, adaboost using decision trees performs better using 11 features rather than the top 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [2, 3, 5], 'base_estimator__min_samples_split': [50, 60, 70]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator, algorithm = 'SAMME')\n",
    "param_grid = {'base_estimator__min_samples_split' : [50,60,70],\n",
    "              'n_estimators' : [2,3,5]\n",
    "             }\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "grid = GridSearchCV(estimator=ada, param_grid=param_grid, cv = cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_estimators': 3, 'base_estimator__min_samples_split': 70} with a score of 0.476\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=3, random_state=None)\n",
      "\tAccuracy: 0.79736\tPrecision: 0.34075\tRecall: 0.12250\tF1: 0.18021\tF2: 0.14050\n",
      "\tTotal predictions: 11000\tTrue positives:  245\tFalse positives:  474\tFalse negatives: 1755\tTrue negatives: 8526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with best features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "min_samples_split = 70\n",
    "n_estimators = 3\n",
    "dtc = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "clf = AdaBoostClassifier(base_estimator=dtc, n_estimators=n_estimators)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  1.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [3, 5, 7], 'base_estimator__min_samples_split': [40, 50, 60, 70]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search using 11 features\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "ada = AdaBoostClassifier(base_estimator=base_estimator, algorithm = 'SAMME')\n",
    "param_grid = {'base_estimator__min_samples_split' : [40,50,60,70],\n",
    "              'n_estimators' : [3,5,7]\n",
    "             }\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "grid = GridSearchCV(estimator=ada, param_grid=param_grid, cv = cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_estimators': 5, 'base_estimator__min_samples_split': 50} with a score of 0.393666666667\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=5, random_state=None)\n",
      "\tAccuracy: 0.84733\tPrecision: 0.40729\tRecall: 0.31850\tF1: 0.35746\tF2: 0.33302\n",
      "\tTotal predictions: 15000\tTrue positives:  637\tFalse positives:  927\tFalse negatives: 1363\tTrue negatives: 12073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with 11 features\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "min_samples_split = 50\n",
    "n_estimators=5\n",
    "dtc = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "clf = AdaBoostClassifier(base_estimator=dtc, n_estimators=n_estimators)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Precision: 0.38034\tRecall: 0.14700\tF1: 0.21204\n",
    "Random forest with 11 features does worse than Adaboost with decision trees and 11 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  1.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [40, 50, 60], 'n_estimators': [2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#grid search using 11 features\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid = {'min_samples_split' : [40, 50, 60],\n",
    "              'n_estimators' : [2,3,4]\n",
    "             }\n",
    "grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'min_samples_split': 50, 'n_estimators': 2} with a score of 0.198333333333\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=2, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.85433\tPrecision: 0.38034\tRecall: 0.14700\tF1: 0.21204\tF2: 0.16756\n",
      "\tTotal predictions: 15000\tTrue positives:  294\tFalse positives:  479\tFalse negatives: 1706\tTrue negatives: 12521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run test on best parameters\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "min_samples_split = 50\n",
    "n_estimators = 2\n",
    "clf = RandomForestClassifier(min_samples_split=min_samples_split,\n",
    "                            n_estimators=n_estimators)\n",
    "test_classifier(clf=clf,dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost with logistic regression\n",
    "\n",
    "Precision: 0.48645\tRecall: 0.18850\tF1: 0.27171\n",
    "\n",
    "Adaboost with logistic regression does a little bit better when using just the top 3 features as opposed to 11.\n",
    "\n",
    "This posting pointed out that the Adaboost's base estimator should support class probabilities, and gave logistic regression as an example.\n",
    "\n",
    "#### Reference \n",
    "http://stackoverflow.com/questions/27107205/sklearn-ensemble-adaboostclassifier-cannot-accecpt-svm-as-base-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'base_estimator__C': [22500.0, 50000.0, 75000.0], 'n_estimators': [2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "algorithm = 'SAMME'\n",
    "reg = LogisticRegression()\n",
    "ada = AdaBoostClassifier(base_estimator=reg, \n",
    "                         algorithm = 'SAMME')\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid = {\n",
    "    'base_estimator__C' : [2.25e4, 5e4, 7.5e4],\n",
    "    'n_estimators' : [2,3,4]\n",
    "}\n",
    "grid = GridSearchCV(estimator=ada, param_grid=param_grid, cv=cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'base_estimator__C': 22500.0, 'n_estimators': 4} with a score of 0.207666666667\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=LogisticRegression(C=22500, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "          learning_rate=1.0, n_estimators=4, random_state=None)\n",
      "\tAccuracy: 0.81627\tPrecision: 0.48645\tRecall: 0.18850\tF1: 0.27171\tF2: 0.21481\n",
      "\tTotal predictions: 11000\tTrue positives:  377\tFalse positives:  398\tFalse negatives: 1623\tTrue negatives: 8602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test with top features\n",
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "base_estimator = LogisticRegression(C=22500)\n",
    "clf = AdaBoostClassifier(base_estimator=base_estimator,\n",
    "                         n_estimators=4,\n",
    "                         algorithm = 'SAMME')\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  1.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'base_estimator__C': [22500.0, 50000.0, 75000.0], 'n_estimators': [2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "algorithm = 'SAMME'\n",
    "reg = LogisticRegression()\n",
    "ada = AdaBoostClassifier(base_estimator=reg, \n",
    "                         algorithm = 'SAMME')\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid = {\n",
    "    'base_estimator__C' : [2.25e4, 5e4, 7.5e4],\n",
    "    'n_estimators' : [2,3,4]\n",
    "}\n",
    "grid = GridSearchCV(estimator=ada, param_grid=param_grid, cv=cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'base_estimator__C': 75000.0, 'n_estimators': 4} with a score of 0.251857142857\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=LogisticRegression(C=75000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "          learning_rate=1.0, n_estimators=4, random_state=None)\n",
      "\tAccuracy: 0.83167\tPrecision: 0.28216\tRecall: 0.17000\tF1: 0.21217\tF2: 0.18468\n",
      "\tTotal predictions: 15000\tTrue positives:  340\tFalse positives:  865\tFalse negatives: 1660\tTrue negatives: 12135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_poi_ratio_scaled',\n",
    "                 'from_poi_ratio_scaled',\n",
    "                 'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "base_estimator = LogisticRegression(C=75000)\n",
    "clf = AdaBoostClassifier(base_estimator=base_estimator,\n",
    "                         n_estimators=4,\n",
    "                         algorithm = 'SAMME')\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of created features\n",
    "\n",
    "Compare SVM with the email ratio features, with just the email counts, and without the email data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with email ratio features\n",
    "Precision: 0.44888\tRecall: 0.73100\tF1: 0.55621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.78218\tPrecision: 0.44614\tRecall: 0.82000\tF1: 0.57787\tF2: 0.70230\n",
      "\tTotal predictions: 11000\tTrue positives: 1640\tFalse positives: 2036\tFalse negatives:  360\tTrue negatives: 6964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['poi',\n",
    "                'to_poi_ratio_scaled',\n",
    "                'bonus_scaled',\n",
    "                'shared_poi_ratio_scaled'\n",
    "                ]\n",
    "C = 1000 \n",
    "gamma = 0.1\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM without new features\n",
    "\n",
    "Precision: 0.18282\tRecall: 0.11600\tF1: 0.14194\n",
    "\n",
    "I used original features, ran feature ranking using random forest, and used the top four from the ranking:\n",
    "\n",
    "[(0.16, 'bonus_scaled'), (0.14, 'exercised_stock_options_scaled'), (0.11, 'from_poi_to_this_person'), (0.1, 'salary_scaled'), (0.08, 'shared_receipt_with_poi'), (0.08, 'other_scaled'), (0.08, 'from_messages'), (0.07, 'expenses_scaled'), (0.05, 'total_payments_scaled'), (0.05, 'from_this_person_to_poi'), (0.03, 'restricted_stock_scaled'), (0.02, 'to_messages'), (0.01, 'total_stock_value_scaled')]\n",
    "\n",
    "I tuned parameters for SVM and tested.  The model performs worse without the new email ratio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature rank using random forest\n",
      "[(0.16, 'bonus_scaled'), (0.14, 'exercised_stock_options_scaled'), (0.11, 'from_poi_to_this_person'), (0.1, 'salary_scaled'), (0.08, 'shared_receipt_with_poi'), (0.08, 'other_scaled'), (0.08, 'from_messages'), (0.07, 'expenses_scaled'), (0.05, 'total_payments_scaled'), (0.05, 'from_this_person_to_poi'), (0.03, 'restricted_stock_scaled'), (0.02, 'to_messages'), (0.01, 'total_stock_value_scaled')]\n"
     ]
    }
   ],
   "source": [
    "#first use random forest to rank all original features\n",
    "\n",
    "feature_list = ['poi',\n",
    "                 'salary_scaled',\n",
    "                 'total_stock_value_scaled',\n",
    "                 'total_payments_scaled',\n",
    "                 'restricted_stock_scaled',\n",
    "                 'exercised_stock_options_scaled',\n",
    "                 'other_scaled',\n",
    "                 'bonus_scaled',\n",
    "                 'expenses_scaled',\n",
    "                 'to_messages',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'shared_receipt_with_poi'\n",
    "                ]\n",
    "\n",
    "feature_only = feature_list[1:]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(features, labels)\n",
    "feature_rank = sorted (zip (map(lambda x: round(x,2),clf.feature_importances_) , feature_only), reverse=True)\n",
    "print \"feature rank using random forest\"\n",
    "print feature_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  1.  0.], n_iter=100, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [500.0, 1000.0, 5000.0, 10000.0], 'gamma': [0.5, 1, 1.5, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "feature_list = ['poi',\n",
    "                'bonus_scaled',\n",
    "                'exercised_stock_options_scaled',\n",
    "                'from_poi_to_this_person',\n",
    "                'salary_scaled'\n",
    "                ]\n",
    "data = featureFormat(my_dataset, feature_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "n_iter = 100\n",
    "cv = StratifiedShuffleSplit(y=labels, n_iter=n_iter, random_state=42)\n",
    "param_grid = {'C': [5e2, 1e3, 5e3, 1e4],\n",
    "              'gamma': [0.5, 1, 1.5, 2]\n",
    "             }\n",
    "grid = GridSearchCV(estimator=svc, param_grid=param_grid, cv=cv, scoring='f1')\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 500.0, 'gamma': 0.5} with a score of 0.127523809524\n"
     ]
    }
   ],
   "source": [
    "print \"The best parameters are {} with a score of {}\".format(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=500, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.78423\tPrecision: 0.18282\tRecall: 0.11600\tF1: 0.14194\tF2: 0.12515\n",
      "\tTotal predictions: 13000\tTrue positives:  232\tFalse positives: 1037\tFalse negatives: 1768\tTrue negatives: 9963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test with parameters tuned for the two best non-email features\n",
    "feature_list = ['poi',\n",
    "                'bonus_scaled',\n",
    "                'exercised_stock_options_scaled',\n",
    "                'from_poi_to_this_person',\n",
    "                'salary_scaled'\n",
    "                ]\n",
    "C = 500\n",
    "gamma = 0.5\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
    "test_classifier(clf=clf, dataset=my_dataset, feature_list=feature_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
